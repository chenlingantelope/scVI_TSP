{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 layers, good but awful results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "from anndata import read_h5ad\n",
    "import sklearn.metrics as met\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/data/yosef2/users/chenling/tabula-sapiens_old/lung_eval\")\n",
    "\n",
    "from lung_eval import *\n",
    "\n",
    "sys.path.append(\"/data/yosef2/users/chenling/cellontology/\")\n",
    "ont_path = \"/data/yosef2/users/chenling/TabulaSapiensData/ontology/\"\n",
    "from cellontology import CellOntolgy\n",
    "\n",
    "from utils import (\n",
    "    import_lung_data,\n",
    "    get_node_to_leaves,\n",
    "    get_names,\n",
    "    get_to_idx_mappers,\n",
    "    get_high_variable_genes,\n",
    "    get_leaf_sampling_probs,\n",
    "    get_classification_performance,\n",
    "    predict_meta,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "important keys\n",
    "\n",
    "obs\n",
    "- labelled (train)\n",
    "- unlabelled (unlabelled examples, for semi supervised methods only)\n",
    "- eval (query data, should not be useful)\n",
    "\n",
    "general\n",
    "- gt: int labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(mode):\n",
    "    \"\"\"Returns train & test batches\n",
    "    \n",
    "    \"\"\"\n",
    "    # TS -> CA\n",
    "    if mode == 0:\n",
    "        train_batch_indices = [\n",
    "            \"TSP1_10X\",\n",
    "            \"TSP1_smartseq2\",\n",
    "            \"TSP2_10X\",\n",
    "            \"TSP2_smartseq2\",\n",
    "        ]\n",
    "        test_batch_indices = [\"Atlas_droplet\", \"Atlas_facs\"]\n",
    "\n",
    "    # TS -> Reyfman\n",
    "    elif mode == 1:\n",
    "        train_batch_indices = [\n",
    "            \"TSP1_10X\",\n",
    "            \"TSP1_smartseq2\",\n",
    "            \"TSP2_10X\",\n",
    "            \"TSP2_smartseq2\",\n",
    "        ]\n",
    "        test_batch_indices = [\"Reyfman\"]\n",
    "\n",
    "    # TS -> Barga\n",
    "    elif mode == 2:\n",
    "        train_batch_indices = [\n",
    "            \"TSP1_10X\",\n",
    "            \"TSP1_smartseq2\",\n",
    "            \"TSP2_10X\",\n",
    "            \"TSP2_smartseq2\",\n",
    "        ]\n",
    "        test_batch_indices = [\"Barga\"]\n",
    "\n",
    "    # CA -> Reyfman\n",
    "    elif mode == 3:\n",
    "        train_batch_indices = [\"Atlas_droplet\", \"Atlas_facs\"]\n",
    "        test_batch_indices = [\"Reyfman\"]\n",
    "\n",
    "    # CA -> Barga\n",
    "    elif mode == 4:\n",
    "        train_batch_indices = [\"Atlas_droplet\", \"Atlas_facs\"]\n",
    "        test_batch_indices = [\"Barga\"]\n",
    "    else:\n",
    "        raise ValueError\n",
    "    return \"\".join(train_batch_indices), \"\".join(test_batch_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "\n",
    "# parser = argparse.ArgumentParser(description=\"Process some integers.\")\n",
    "# parser.add_argument(\"--mode\", type=int, default=0)\n",
    "# parser.add_argument(\"--prefix\", type=str, default='results')\n",
    "# args = parser.parse_args()\n",
    "# mode = int(args.mode)\n",
    "# prefix = str(args.prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 4\n",
    "prefix = \"results3layer\"\n",
    "TEST_SPLIT_SEED = 2\n",
    "ON_LEAVES_ONLY = False\n",
    "\n",
    "SAVE_DIR = \"/data/yosef2/users/chenling/TabulaSapiensData/hierarchial_scANVI/scVI_TSP/experiments/lung_chenling/\"\n",
    "file_format = \"%s_train{}_test{}_leavesonly{}_{}3.h5ad\" % prefix\n",
    "\n",
    "prefix = \"results1layer\"\n",
    "file_format = \"%s_train{}_test{}_leavesonly{}_{}_B50epochs_kl1.h5ad\" % prefix\n",
    "\n",
    "\n",
    "prefix = \"results1layer\"\n",
    "file_format = \"%s_train{}_test{}_leavesonly{}_{}_C50epochs_kl1.h5ad\" % prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = '/data/yosef2/users/chenling/TabulaSapiensData/hierarchial_scANVI/scVI_TSP/experiments/lung_chenling/results1layer_trainAtlas_dropletAtlas_facs_testBarga_leavesonlyFalse_2_C50epochs_kl1.h5ad', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-688b99f3f0d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     ),\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfinal_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_h5ad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/yosef2/users/pierreboyeau/miniconda3/lib/python3.7/site-packages/anndata-0.7.4-py3.7.egg/anndata/_io/h5ad.py\u001b[0m in \u001b[0;36mread_h5ad\u001b[0;34m(filename, backed, as_sparse, as_sparse_fmt, chunk_size)\u001b[0m\n\u001b[1;32m    398\u001b[0m     )\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/yosef2/users/pierreboyeau/miniconda3/lib/python3.7/site-packages/h5py-2.10.0-py3.7-linux-x86_64.egg/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/yosef2/users/pierreboyeau/miniconda3/lib/python3.7/site-packages/h5py-2.10.0-py3.7-linux-x86_64.egg/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = '/data/yosef2/users/chenling/TabulaSapiensData/hierarchial_scANVI/scVI_TSP/experiments/lung_chenling/results1layer_trainAtlas_dropletAtlas_facs_testBarga_leavesonlyFalse_2_C50epochs_kl1.h5ad', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# Import anndata based on scenario `mode`\n",
    "train_batch_indices, test_batch_indices = get_train_test(mode=mode)\n",
    "data_file = os.path.join(\n",
    "    SAVE_DIR,\n",
    "    file_format.format(\n",
    "        train_batch_indices, test_batch_indices, ON_LEAVES_ONLY, TEST_SPLIT_SEED\n",
    "    ),\n",
    ")\n",
    "final_data = read_h5ad(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ontology info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets(annd):\n",
    "    \"\"\"Extracts cell-type GT labels\n",
    "    \n",
    "    \"\"\"\n",
    "    celltype_dict = {}\n",
    "    for x in adj.nodes:\n",
    "        celltype_dict[adj.nodes[x][\"name\"]] = x\n",
    "\n",
    "    ontology_id = [celltype_dict[x] for x in annd.obs[\"free_annotation\"]]\n",
    "    return nodes_to_indices[ontology_id].values\n",
    "\n",
    "\n",
    "def obtain_maps():\n",
    "    \"\"\"Computes maps between \n",
    "     1. ids (0, 1, 2, ... 100, ..)\n",
    "     2. nodes (CI:XXXXXX) ==> cellontology-based\n",
    "     3. cell-type names (\"B-cell\")\n",
    "    \n",
    "    \"\"\"\n",
    "    all_to_leaves = get_node_to_leaves(adjm)\n",
    "    leaf_node_names = list(adjm[-1].columns.values) + [\"low_quality\"] + [\"unassigned\"]\n",
    "    other_node_names = list(\n",
    "        all_to_leaves.index[~np.isin(all_to_leaves.index, leaf_node_names)]\n",
    "    )\n",
    "    id_to_label, label_to_id = get_names(all_to_leaves=all_to_leaves, adj=adj)\n",
    "    nodes_to_indices, indices_to_nodes = get_to_idx_mappers(\n",
    "        leaf_node_names=leaf_node_names, other_node_names=other_node_names,\n",
    "    )\n",
    "    return dict(\n",
    "        id_to_label=id_to_label,\n",
    "        label_to_id=label_to_id,\n",
    "        nodes_to_indices=nodes_to_indices,\n",
    "        indices_to_nodes=indices_to_nodes,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load adjacency property and matrix\n",
    "adj = pkl.load(\n",
    "    open(\n",
    "        \"/data/yosef2/users/chenling/TabulaSapiensData/ontology/ontology.lung.2.flat.pkl\",\n",
    "        \"rb\",\n",
    "    )\n",
    ")\n",
    "adjm = adj.adjacency_matrix()\n",
    "\n",
    "maps = obtain_maps()\n",
    "id_to_label = maps[\"id_to_label\"]\n",
    "label_to_id = maps[\"label_to_id\"]\n",
    "nodes_to_indices = maps[\"nodes_to_indices\"]\n",
    "indices_to_nodes = maps[\"indices_to_nodes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.obs[\"predictions_names_META\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load scnet results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(scnet_obs[\"scnet\"] == \"rand\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scnet_format = \"results_train{}_test{}_{}new_fullgenFalse/obs.scnet.csv\"\n",
    "scnet_file = os.path.join(\n",
    "    SAVE_DIR,\n",
    "    scnet_format.format(train_batch_indices, test_batch_indices, ON_LEAVES_ONLY),\n",
    ")\n",
    "\n",
    "scnet_obs = pd.read_csv(scnet_file, index_col=1)\n",
    "assert np.sum(scnet_obs.index == final_data.obs.index) == final_data.shape[0]\n",
    "final_data.obs[\"predictions_names_scnet\"] = id_to_label.reindex(\n",
    "    indices_to_nodes.reindex(\n",
    "        scnet_obs[\"scnet\"]\n",
    "        .replace(\"rand\", 0)\n",
    "        .astype(int)\n",
    "#         convert_objects(convert_numeric=True)\n",
    "    )\n",
    ").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treemaper is a DataFrame that maps any node to its ancestors\n",
    "# For a given cell_type a, di correspond to:\n",
    "# - a if node a is di <= depth(a)\n",
    "# - the ancestor of a at depth i\n",
    "# d4 corresponds to the more general cell-types, d0 to the more specific\n",
    "\n",
    "tree_mapper = []\n",
    "for idx, node in (indices_to_nodes).iteritems():\n",
    "    _node = node\n",
    "    path = []\n",
    "    for mat in adjm[::-1]:\n",
    "        if _node in mat.columns:\n",
    "            anc = mat.loc[:, _node].argmax()\n",
    "        else:\n",
    "            anc = _node\n",
    "        path.append(anc)\n",
    "        _node = anc\n",
    "    tree_path = {\"d{}\".format(i): nod for i, nod in enumerate(path)}\n",
    "    tree_mapper.append(dict(node=node, **tree_path,))\n",
    "tree_mapper = pd.DataFrame(tree_mapper).set_index(\"node\").assign(orig=lambda x: x.index)\n",
    "tree_mapper.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dictionary of matched cell types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import obonet\n",
    "\n",
    "obo = ont_path + 'cl.obo.txt'\n",
    "f = open(obo, \"r\")\n",
    "ontology = obonet.read_obo(f)\n",
    "f.close()\n",
    "\n",
    "match = {}\n",
    "for x in ontology.nodes:\n",
    "    o = nx.ancestors(ontology, x)\n",
    "    p = nx.descendants(ontology, x)\n",
    "    sibs = []\n",
    "    parents = ontology.successors(x)\n",
    "    for y in parents:\n",
    "        sibs += list(ontology.successors(y))\n",
    "    o = [ontology.nodes[i]['name'] for i in o]\n",
    "    p = [ontology.nodes[i]['name'] for i in p]\n",
    "    x = ontology.nodes[x]['name']\n",
    "#     match[x] = [x] + list(o) + list(p) + sibs\n",
    "    match[x] = [x] + list(o) + list(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and formatting functions metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_rows(row):\n",
    "    \"\"\"Format (in dollars) metrics, round them and bolder the higher one\n",
    "    \n",
    "    \"\"\"\n",
    "    max_is_best = row.pop(\"max_is_best\")\n",
    "    #     print(row)\n",
    "    row = row.astype(float)\n",
    "    if max_is_best:\n",
    "        where_max = np.argmax(np.array(row.values))\n",
    "    else:\n",
    "        where_max = np.argmin(np.array(row.values))\n",
    "    row = row.apply(lambda x: \"{0:.3f}\".format(x))\n",
    "    row.iloc[where_max] = \"\\\\mathbf{{ {} }}\".format(row.iloc[where_max])\n",
    "    row = row.apply(lambda x: \"${}$\".format(x))\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation function Chenling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConfusionMatrix(y_true, y_pred, y_ref):\n",
    "    \"\"\" Compute per cell type confusion matrix and cell type proportions\n",
    "    \n",
    "    \"\"\"\n",
    "    celltypes, prop = np.unique(y_true, return_counts=True)\n",
    "    ref_prop = [np.mean(y_ref == x) for x in celltypes]\n",
    "    prop = prop / np.sum(prop)\n",
    "    mtx = confusion_matrix(y_true, y_pred, normalize=\"true\", labels=celltypes)\n",
    "    mtx = pd.DataFrame(mtx, columns=celltypes, index=celltypes)\n",
    "    return (\n",
    "        pd.DataFrame(\n",
    "            [celltypes, prop, ref_prop], index=[\"celltype\", \"prop\", \"ref_prop\"]\n",
    "        ).T,\n",
    "        mtx,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PerCelltypeAccuracy(confusion_matrix, prop, match):\n",
    "    \"\"\" Compute per celltype accuracy \n",
    "    \n",
    "    \"\"\"\n",
    "    acc1 = []\n",
    "    acc2 = []\n",
    "    acc_detail = {}\n",
    "    for x in prop['celltype']:\n",
    "        m = match[x]\n",
    "        print(x)\n",
    "\n",
    "        pred = confusion_matrix.loc[x]\n",
    "        acc1.append(pred[x])\n",
    "        acc2.append(pred.reindex(m).sum())\n",
    "        temp1 = pd.DataFrame(pred.reindex(m).values, index=['m '+x for x in m], columns=[x])\n",
    "        others = [x for x in confusion_matrix.columns[pred>0] if x not in m]\n",
    "        temp2 = pd.DataFrame(pred[others].values, index=others, columns=[x])\n",
    "        temp = pd.concat([temp1, temp2])\n",
    "        temp.dropna(inplace=True)\n",
    "        acc_detail[x] = temp\n",
    "    acc = deepcopy(prop)\n",
    "    acc['acc'] = acc1\n",
    "    acc['hier_acc'] = acc2\n",
    "    return acc_detail, acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stacked barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_heatmap(mtx, save=False, res_dir=None, filename=None):\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    sns.heatmap(mtx, linewidths=0.005, cmap=\"OrRd\")\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig(res_dir + filename)\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_barplot(acc, save=False, res_dir=None, filename=None):\n",
    "    acc.sort_values('prop', ascending=False, inplace=True)\n",
    "    sns.set()\n",
    "    plot = deepcopy(acc)\n",
    "    plot.set_index('celltype', inplace=True)\n",
    "    plot = plot[['acc', 'hier_acc']]\n",
    "    plot['hier_acc'] = plot['hier_acc'] - plot['acc']\n",
    "    plot.plot(kind='bar', stacked=True, figsize=(10,6))\n",
    "    plt.legend(bbox_to_anchor=(1, 0.9))\n",
    "    plt.ylabel('Prediction Accuracy')\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig(res_dir + filename)\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## confusion matrix heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"kNN\",\n",
    "    \"scANVI\",\n",
    "#     \"scVI_scANVI\",\n",
    "    #     \"hscANVI (leaves)\",\n",
    "        \"hscANVI (full)\",\n",
    "    \"SVM\",\n",
    "    \"META\",\n",
    "    \"scnet\",\n",
    "]\n",
    "preds_key_fmt = \"predictions_names_{}\"\n",
    "pred_keys = [preds_key_fmt.format(model) for model in model_names]\n",
    "reference_key = \"free_annotation\"\n",
    "ari_key = \"leiden\"\n",
    "\n",
    "models_renamer = {predname: \"_\".join(predname.split(\"_\")[2:]) for predname in pred_keys}\n",
    "# \"predictions_ids_{}\"\n",
    "# \"predictions_nodes_{}\"\n",
    "# \"predictions_names_{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if \"X_umap\" not in final_data.obsm_keys():\n",
    "#     sc.pp.neighbors(final_data, n_neighbors=30, n_pcs=50, use_rep=\"scvi_latent\")\n",
    "#     sc.tl.umap(final_data)\n",
    "#     final_data.write_h5ad(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc(adata, y_pred, y_true, y_ari, y_ref, where_eval, where_leaves, method, plotting=False):\n",
    "    plot_dir = data_file.replace(\".h5ad\", \"\")\n",
    "    if not os.path.isdir(plot_dir):\n",
    "        os.mkdir(plot_dir)\n",
    "    # Adjusted Rand index based on reference `y_ari`\n",
    "    \n",
    "    adata.obs['pred'] = y_pred\n",
    "    # Overall accuracy\n",
    "    excluded = y_pred.isna()\n",
    "    if excluded.mean()>0:\n",
    "        print(excluded.mean())\n",
    "    where_eval = where_eval & ~excluded\n",
    "    if plotting:\n",
    "        fig = sc.pl.umap(adata[where_eval], color=[\"pred\"], return_fig=True)\n",
    "        fig.savefig(plot_dir + \"/test.%s.umap.pdf\"%method, bbox_inches=\"tight\")\n",
    "\n",
    "    prop, mtx = ConfusionMatrix(y_true[where_eval], y_pred[where_eval], y_ref)\n",
    "    confusion_matrix_heatmap(\n",
    "        mtx, save=True, res_dir=plot_dir, filename=\"/test.%s.heatmap.pdf\"%method\n",
    "    )\n",
    "\n",
    "    acc_detail, acc = PerCelltypeAccuracy(mtx, prop, match)\n",
    "    if plotting:\n",
    "        accuracy_barplot(acc.loc[acc[\"ref_prop\"] > 0], save=True, res_dir=plot_dir, filename=\"/test.%s.inref.barplot.pdf\"%method)\n",
    "        accuracy_barplot(acc.loc[acc[\"ref_prop\"] == 0], save=True, res_dir=plot_dir, filename=\"/test.%s.outref.barplot.pdf\"%method)\n",
    "\n",
    "    # Accuracy on leaves\n",
    "    where_leaves_eval = where_eval & where_leaves\n",
    "    #     where_leaves_eval = where_leaves\n",
    "\n",
    "    if plotting:\n",
    "        fig = sc.pl.umap(adata[where_leaves_eval], color=[\"pred\"], return_fig=True)\n",
    "        fig.savefig(plot_dir + \"/leaves.%s.umap.pdf\"%method, bbox_inches=\"tight\")\n",
    "\n",
    "    prop, mtx = ConfusionMatrix(\n",
    "        y_true[where_leaves_eval], y_pred[where_leaves_eval], y_ref\n",
    "    )\n",
    "    confusion_matrix_heatmap(\n",
    "        mtx, save=True, res_dir=plot_dir, filename=\"/leaves.%s.heatmap.pdf\"%method\n",
    "    )\n",
    "\n",
    "    lacc_detail, lacc = PerCelltypeAccuracy(mtx, prop, match)\n",
    "    if plotting:\n",
    "        accuracy_barplot(lacc.loc[lacc[\"ref_prop\"] > 0], save=True, res_dir=plot_dir, filename=\"/testl.%s.inref.barplot.pdf\"%method)\n",
    "        accuracy_barplot(lacc.loc[lacc[\"ref_prop\"] == 0], save=True, res_dir=plot_dir, filename=\"/testl.%s.outref.barplot.pdf\"%method)\n",
    "\n",
    "    return acc, acc_detail, lacc, lacc_detail\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_procedure(y_pred, y_true, y_ari, y_ref, where_eval, where_leaves):\n",
    "    plot_dir = data_file.replace(\".h5ad\", \"\")\n",
    "    if not os.path.isdir(plot_dir):\n",
    "        os.mkdir(plot_dir)\n",
    "    # Adjusted Rand index based on reference `y_ari`\n",
    "    excluded = y_pred.isna()\n",
    "    if excluded.mean()>0:\n",
    "        print(excluded.mean())\n",
    "    where_eval = where_eval & ~excluded\n",
    "    ari = met.adjusted_rand_score(\n",
    "        labels_true=y_ari[where_eval], labels_pred=y_pred[where_eval]\n",
    "    )\n",
    "    \n",
    "    # Overall accuracy\n",
    "    prop, mtx = ConfusionMatrix(y_true[where_eval], y_pred[where_eval], y_ref)\n",
    "    acc_detail, acc = PerCelltypeAccuracy(mtx, prop, match)\n",
    "    \n",
    "    # Accuracy on leaves\n",
    "    where_leaves_eval = where_eval & where_leaves\n",
    "    #     where_leaves_eval = where_leaves\n",
    "\n",
    "    prop, mtx = ConfusionMatrix(\n",
    "        y_true[where_leaves_eval], y_pred[where_leaves_eval], y_ref\n",
    "    )\n",
    "\n",
    "    lacc_detail, lacc = PerCelltypeAccuracy(mtx, prop, match)\n",
    "\n",
    "    is_out = ~np.isin(y_pred[where_eval], np.unique(y_true[where_eval]))\n",
    "    poe = is_out.mean()\n",
    "    res = {\n",
    "        \"ARI\": ari,\n",
    "        \"w.Acc\": acc[\"acc\"].mean(),\n",
    "        \"w.l. Acc\": lacc[\"acc\"].mean(),\n",
    "        \"w.h. Acc\": acc[\"hier_acc\"].mean(),\n",
    "        \"w.h.l. Acc\": lacc[\"hier_acc\"].mean(),\n",
    "        \"POE\": poe,\n",
    "    }\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __y_true=final_data.obs[reference_key].values\n",
    "# __y_ari=final_data.obs[ari_key].values\n",
    "# __y_ref=final_data.obs.loc[\n",
    "# final_data.obs[\"labelled\"].values == \"True\", reference_key\n",
    "# ].values\n",
    "# __where_eval=(final_data.obs[\"eval\"] == \"True\").values\n",
    "# __where_leaves=(final_data.obs[\"eval_leaves\"] == \"True\").values\n",
    "\n",
    "# __y_pred = final_data.obs[pred_keys].iloc[:, 0]\n",
    "\n",
    "# prop, mtx = ConfusionMatrix(__y_true[__where_eval], __y_pred[__where_eval], __y_ref)\n",
    "# confusion_matrix_heatmap(\n",
    "#     mtx, save=True, res_dir=\"savedir\", filename=\"/test.%s.heatmap.pdf\"%\"toto\"\n",
    "# )\n",
    "\n",
    "# acc_detail, acc = PerCelltypeAccuracy(mtx, prop, match)\n",
    "\n",
    "# for x in prop['celltype']:\n",
    "#     break\n",
    "# m = match[x]\n",
    "# print(x)\n",
    "\n",
    "# pred = mtx.loc[x]\n",
    "# pred[x]\n",
    "# pred[m].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_f = (\n",
    "    final_data.obs[pred_keys]\n",
    "    .apply(\n",
    "        evaluation_procedure,\n",
    "        y_true=final_data.obs[reference_key].values,\n",
    "        y_ari=final_data.obs[ari_key].values,\n",
    "        y_ref=final_data.obs.loc[\n",
    "            :, #final_data.obs[\"labelled\"].values == \"True\", \n",
    "            reference_key\n",
    "        ].values,\n",
    "        where_eval=(final_data.obs[\"eval\"] == \"True\").values,\n",
    "        where_leaves=(final_data.obs[\"eval_leaves\"] == \"True\").values,\n",
    "        axis=0,\n",
    "        raw=True,\n",
    "    )\n",
    "    .rename(columns=models_renamer)\n",
    "    .loc[\n",
    "    [\n",
    "        \"ARI\",\n",
    "        \"POE\",\n",
    "        \"w.Acc\",\n",
    "        \"w.h. Acc\",\n",
    "    ]\n",
    "]\n",
    "    # Placeholder for singlecellnet, comment as soon as scn is included in the study\n",
    "    .loc[:, \n",
    "         [\n",
    "             \"SVM\", \n",
    "             \"kNN\", \n",
    "             \"scnet\",\n",
    "             \"hscANVI (full)\",\n",
    "             \"scANVI\", \n",
    "#              \"scVI_scANVI\", \n",
    "             \"META\",\n",
    "         ],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2\n",
    "(\n",
    "    table_f.assign(max_is_best=[True, False, True, True]).apply(\n",
    "    format_rows, axis=1\n",
    ")\n",
    ")\n",
    "# Specify for each metric if maximum values are better\n",
    "# Format the numerical values for latex copy/paste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1\n",
    "(\n",
    "    table_f.assign(max_is_best=[True, False, True, True]).apply(\n",
    "    format_rows, axis=1\n",
    ")\n",
    ")\n",
    "# Specify for each metric if maximum values are better\n",
    "# Format the numerical values for latex copy/paste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4\n",
    "(\n",
    "    table_f.assign(max_is_best=[True, False, True, True]).apply(\n",
    "    format_rows, axis=1\n",
    ")\n",
    ")\n",
    "# Specify for each metric if maximum values are better\n",
    "# Format the numerical values for latex copy/paste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3\n",
    "(\n",
    "    table_f.assign(max_is_best=[True, False, True, True]).apply(\n",
    "    format_rows, axis=1\n",
    ")\n",
    ")\n",
    "# Specify for each metric if maximum values are better\n",
    "# Format the numerical values for latex copy/paste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0\n",
    "(\n",
    "    table_f.assign(max_is_best=[True, False, True, True]).apply(\n",
    "    format_rows, axis=1\n",
    ")\n",
    ")\n",
    "# Specify for each metric if maximum values are better\n",
    "# Format the numerical values for latex copy/paste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_row(x):\n",
    "    return x / x.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_f_norm = table_f.apply(norm_row, axis=1)\n",
    "sns.heatmap(table_f_norm, annot=table_f, fmt=\".3f\")\n",
    "plt.savefig(data_file.replace(\".h5ad\", \"\") + \"/table_f.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == 0:\n",
    "    print(\n",
    "        \"\\\\begin{table}[]\\n\"\n",
    "        + \"\\\\centering\\n\"\n",
    "        + \"\\\\begin{tabular}{lrrrrrr}\\n\"\n",
    "        + \"\\\\toprule\\n\"\n",
    "        + \"& \\\\textbf{%s} & \\\\textbf{%s} & \\\\textbf{%s}  & \\\\textbf{%s} & \\\\textbf{%s} & \\\\textbf{%s}\\\\\\\\\\n\"\n",
    "        % (\"SVM\", \"scVI\\\\_kNN\", \"scnet\", \"scANVI\", \"scVI\\\\_scANVI\", \"META\")\n",
    "   )\n",
    "    print(\"\\\\textbf{TSP $\\\\rightarrow$ CA} & &  &  & & \\\\\\\\\\n\")\n",
    "if mode == 1:\n",
    "    print(\"\\\\hline \\n\")\n",
    "    print(\"\\\\textbf{TSP $\\\\rightarrow$ Reyfman} & &  &  & & \\\\\\\\\\n\")\n",
    "if mode == 2:\n",
    "    print(\"\\\\hline \\n\")\n",
    "    print(\"\\\\textbf{TSP $\\\\rightarrow$ Barga} & &  &  & & \\\\\\\\\\n\")\n",
    "if mode == 3:\n",
    "    print(\"\\\\hline \\n\")\n",
    "    print(\"\\\\textbf{CA $\\\\rightarrow$ Reyfman} & &  &  & & \\\\\\\\\\n\")\n",
    "if mode == 4:\n",
    "    print(\"\\\\hline \\n\")\n",
    "    print(\"\\\\textbf{CA $\\\\rightarrow$ Barga} & &  &  & & \\\\\\\\\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric_name, metrics in table_f.iterrows():\n",
    "    print(metric_name, \" & \", \" & \".join(metrics) + \"\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == 4:\n",
    "    print(\n",
    "        \"\\\\bottomrule\\n\"\n",
    "        + \"\\\\end{tabular}\\n\"\n",
    "        + \"\\\\caption {Overall classification performance for interdataset annotation (annotations subsampled to a maximum of 100 labeled examples for each cell-types using %s training}\\n\"\n",
    "        % data_file.split(\"/\")[-1].split(\"_\")[0]\n",
    "        + \"\\\\label{tab}\\n\"\n",
    "        + \"\\\\end{table}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = plot_acc(\n",
    "    adata=final_data,\n",
    "    y_pred=final_data.obs[\"predictions_names_scANVI\"],\n",
    "    y_true=final_data.obs[reference_key].values,\n",
    "    y_ari=final_data.obs[ari_key].values,\n",
    "    y_ref=final_data.obs.loc[\n",
    "        final_data.obs[\"labelled\"].astype(str) == \"True\", reference_key\n",
    "    ].values,\n",
    "    where_eval=(final_data.obs[\"eval\"] == \"True\").values,\n",
    "    where_leaves=(final_data.obs[\"eval_leaves\"] == \"True\").values,\n",
    "    method=\"_\".join(x.split(\"_\")[2:]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in pred_keys:\n",
    "    plot_acc(\n",
    "        adata=final_data,\n",
    "        y_pred=final_data.obs[x],\n",
    "        y_true=final_data.obs[reference_key].values,\n",
    "        y_ari=final_data.obs[ari_key].values,\n",
    "        y_ref=final_data.obs.loc[\n",
    "            final_data.obs[\"labelled\"].astype(str) == \"True\", reference_key\n",
    "        ].values,\n",
    "        where_eval=final_data.obs[\"eval\"].astype(bool).values,\n",
    "        where_leaves=final_data.obs[\"eval_leaves\"].astype(bool).values,\n",
    "        method=\"_\".join(x.split(\"_\")[2:]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"predictions_names_scANVI\"\n",
    "acc, acc_detail, lacc, lacc_detail = plot_acc(\n",
    "    adata=final_data,\n",
    "    y_pred=final_data.obs[x],\n",
    "    y_true=final_data.obs[reference_key].values,\n",
    "    y_ari=final_data.obs[ari_key].values,\n",
    "    y_ref=final_data.obs.loc[\n",
    "        final_data.obs[\"labelled\"].astype(str) == \"True\", reference_key\n",
    "    ].values,\n",
    "    where_eval=final_data.obs[\"eval\"].astype(bool).values,\n",
    "    where_leaves=final_data.obs[\"eval_leaves\"].astype(bool).values,\n",
    "    method=\"_\".join(x.split(\"_\")[2:]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_detail[\"naive thymus-derived CD4-positive, alpha-beta T cell\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_detail[\"dendritic cell, human\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_detail[\"bronchial smooth muscle cell\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "169.767px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
