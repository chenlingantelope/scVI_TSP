{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext nb_black\n",
    "\n",
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/yosef2/users/chenling/tabula-sapiens_old/lung_eval'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import sklearn.metrics as met\n",
    "\n",
    "from utils import (\n",
    "    import_lung_data,\n",
    "    get_node_to_leaves,\n",
    "    get_names,\n",
    "    get_to_idx_mappers,\n",
    "    get_high_variable_genes,\n",
    "    get_leaf_sampling_probs,\n",
    "    get_classification_performance,\n",
    "    predict_meta,\n",
    ")\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scvi.dataset import GeneExpressionDataset\n",
    "from scvi.inference import SemiSupervisedTrainer, UnsupervisedTrainer\n",
    "from scvi.models import SCANVI, VAE\n",
    "\n",
    "SAVE_DIR = \"/data/yosef2/users/chenling/TabulaSapiensData/hierarchial_scANVI/scVI_TSP/experiments/lung_chenling/\"\n",
    "\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)\n",
    "\n",
    "import sklearn.metrics as met\n",
    "import pickle as pkl\n",
    "from anndata import read_h5ad\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook parameters:\n",
    "\n",
    "- `is_fair`: if True, ensures that training data is balanced, by making sure than no more than 100 labelled examples are used for each cell-type\n",
    "- `ON_LEAVES_ONLY`: if True, only the leaves are used to compare the different algorithnms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_fair = True\n",
    "ON_LEAVES_ONLY = False\n",
    "N_EXAMPLES_MAX = 100\n",
    "init = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/data/yosef2/users/chenling/TabulaSapiensData/PublicDatasets/lung/\"\n",
    "combined = read_h5ad(DATA_PATH + \"combined.lung.3000.clean.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description=\"Process some integers.\")\n",
    "# parser.add_argument(\"--mode\", type=int, default=0)\n",
    "# parser.add_argument(\"--test_seed\", type=int, default=0)\n",
    "# args = parser.parse_args()\n",
    "# mode = int(args.mode)\n",
    "# TEST_SPLIT_SEED = int(args.test_seed)\n",
    "\n",
    "mode = 3\n",
    "TEST_SPLIT_SEED = 0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batchname</th>\n",
       "      <th>n_cells</th>\n",
       "      <th>n_celltypes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Atlas_droplet</td>\n",
       "      <td>65662</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Atlas_facs</td>\n",
       "      <td>9409</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Barga</td>\n",
       "      <td>5916</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reyfman</td>\n",
       "      <td>25246</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TSP1_10X</td>\n",
       "      <td>14780</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TSP1_smartseq2</td>\n",
       "      <td>728</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TSP2_10X</td>\n",
       "      <td>20872</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TSP2_smartseq2</td>\n",
       "      <td>782</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        batchname n_cells n_celltypes\n",
       "0   Atlas_droplet   65662          42\n",
       "1      Atlas_facs    9409          38\n",
       "2           Barga    5916          26\n",
       "3         Reyfman   25246          19\n",
       "4        TSP1_10X   14780          34\n",
       "5  TSP1_smartseq2     728          31\n",
       "6        TSP2_10X   20872          34\n",
       "7  TSP2_smartseq2     782          25"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = np.unique(combined.obs[\"batch_names\"], return_counts=True)\n",
    "c = []\n",
    "for x in a:\n",
    "    c.append(\n",
    "        len(\n",
    "            np.unique(combined[combined.obs[\"batch_names\"] == x].obs[\"free_annotation\"])\n",
    "        )\n",
    "    )\n",
    "\n",
    "pd.DataFrame([a, b, c], index=[\"batchname\", \"n_cells\", \"n_celltypes\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(combined.obs[\"free_annotation\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlas_droplet Atlas_facs 75071 43\n",
      "Reyfman 25246 19\n",
      "Barga 5916 26\n",
      "TSP1_10X TSP1_smartseq2 TSP2_10X TSP2_smartseq2 37162 34\n"
     ]
    }
   ],
   "source": [
    "for dataset in [\n",
    "    [\"Atlas_droplet\", \"Atlas_facs\"],\n",
    "    [\"Reyfman\"],\n",
    "    [\"Barga\"],\n",
    "    [\"TSP1_10X\", \"TSP1_smartseq2\", \"TSP2_10X\", \"TSP2_smartseq2\"],\n",
    "]:\n",
    "    print(\n",
    "        \" \".join(dataset),\n",
    "        np.sum(np.isin(combined.obs[\"batch_names\"], dataset)),\n",
    "        len(\n",
    "            np.unique(\n",
    "                combined[np.isin(combined.obs[\"batch_names\"], dataset)].obs[\n",
    "                    \"free_annotation\"\n",
    "                ]\n",
    "            )\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels and ontology processing\n",
    "\n",
    "\n",
    "Loads all the tools necessary to use hscANVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# Load adjacency property and matrix\n",
    "adj = pkl.load(\n",
    "    open(\n",
    "        \"/data/yosef2/users/chenling/TabulaSapiensData/ontology/ontology.lung.2.flat.pkl\",\n",
    "        \"rb\",\n",
    "    )\n",
    ")\n",
    "adjm = adj.adjacency_matrix()\n",
    "\n",
    "adjmc = []\n",
    "for x in adjm:\n",
    "    a = x.shape[0]\n",
    "    b = x.shape[1]\n",
    "    temp = np.zeros((a, b))\n",
    "    temp[:a, :b] = np.asarray(x)\n",
    "    # temp[a, b] = 1\n",
    "    adjmc.append(temp)\n",
    "\n",
    "print(len(adjmc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now construct mappers between the different representations of cell-types:\n",
    "    \n",
    "    - Names: (\"B cell\")\n",
    "    - Tree representation (\"CL:XXXXXXX\")\n",
    "    - Integer representation, used to train the different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructs the dataset labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_to_leaves = get_node_to_leaves(adjm)\n",
    "leaf_node_names = list(adjm[-1].columns.values) + [\"low_quality\"] + [\"unassigned\"]\n",
    "other_node_names = list(\n",
    "    all_to_leaves.index[~np.isin(all_to_leaves.index, leaf_node_names)]\n",
    ")\n",
    "\n",
    "id_to_label, label_to_id = get_names(all_to_leaves=all_to_leaves, adj=adj)\n",
    "nodes_to_indices, indices_to_nodes = get_to_idx_mappers(\n",
    "    leaf_node_names=leaf_node_names, other_node_names=other_node_names,\n",
    ")\n",
    "\n",
    "nodes = label_to_id[combined.obs[\"free_annotation\"]]\n",
    "labels_f = nodes_to_indices[label_to_id[combined.obs[\"free_annotation\"]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct table of leaf frequencies, used to sample leave labels from intermediate labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/yosef2/users/chenling/TabulaSapiensData/hierarchial_scANVI/scVI_TSP/utils.py:177: RuntimeWarning: invalid value encountered in true_divide\n",
      "  1, keepdims=True\n",
      "/data/yosef2/users/pierreboyeau/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "flat_leaves = []\n",
    "for li in all_to_leaves[nodes].tolist():\n",
    "    flat_leaves += li\n",
    "flat_leaves = pd.Series(flat_leaves)\n",
    "node_counts = flat_leaves.groupby(flat_leaves).size()\n",
    "leaf_counts = node_counts.reindex(leaf_node_names).fillna(0.0)\n",
    "\n",
    "nodes_to_leaves_probs = get_leaf_sampling_probs(\n",
    "    all_to_leaves=all_to_leaves,\n",
    "    leaf_counts=leaf_counts,\n",
    "    nodes_to_indices=nodes_to_indices,\n",
    ")\n",
    "nodes_to_leaves_probs = torch.tensor(nodes_to_leaves_probs, device=\"cuda\")\n",
    "nodes_to_leaves_probs_cpu = torch.tensor(nodes_to_leaves_probs, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The h5ad data used in this notebook contains 7 batches:\n",
    "\n",
    "    0 reyman_ann\n",
    "    1 barga_ann\n",
    "    2 lung_droplet_ann\n",
    "    3 lung_facs_ann\n",
    "    4 lung_ts1_10x\n",
    "    5 lung_ts1_facs\n",
    "    6 lung_ts2_10x\n",
    "    7 lung_ts2_facs\n",
    "    \n",
    "\n",
    "    I labelled in unlabelled\n",
    "    II unlabelled in labelled\n",
    "    III other\n",
    "    \n",
    "    \n",
    "    CA Reyfman more II than I\n",
    "    CA Barga more II than I\n",
    "    TS Reyfman more II than I\n",
    "    TS Barga III\n",
    "    TS CA Approximately the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/data/yosef2/users/chenling/TabulaSapiensData/hierarchial_scANVI/scVI_TSP/experiments/lung_chenling/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TS -> CA\n",
    "if mode == 0:\n",
    "    train_batch_indices = [\"TSP1_10X\", \"TSP1_smartseq2\", \"TSP2_10X\", \"TSP2_smartseq2\"]\n",
    "    test_batch_indices = [\"Atlas_droplet\", \"Atlas_facs\"]\n",
    "\n",
    "# TS -> Reyfman\n",
    "elif mode == 1:\n",
    "    train_batch_indices = [\"TSP1_10X\", \"TSP1_smartseq2\", \"TSP2_10X\", \"TSP2_smartseq2\"]\n",
    "    test_batch_indices = [\"Reyfman\"]\n",
    "    \n",
    "\n",
    "# TS -> Barga\n",
    "elif mode == 2:\n",
    "    train_batch_indices = [\"TSP1_10X\", \"TSP1_smartseq2\", \"TSP2_10X\", \"TSP2_smartseq2\"]\n",
    "    test_batch_indices = [\"Barga\"]\n",
    "\n",
    "\n",
    "# CA -> Reyfman\n",
    "elif mode == 3:\n",
    "    train_batch_indices = [\"Atlas_droplet\", \"Atlas_facs\"]\n",
    "    test_batch_indices = [\"Reyfman\"]\n",
    "    \n",
    "\n",
    "# CA -> Barga                                  \n",
    "elif mode == 4:\n",
    "    train_batch_indices = [\"Atlas_droplet\", \"Atlas_facs\"]\n",
    "    test_batch_indices = [\"Barga\"]\n",
    "\n",
    "\n",
    "all_batch_indices = train_batch_indices + test_batch_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## construct cell type labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of subset of indices : 100317\n",
      "subsampled eval 25246\n",
      "labels_final (100317,)\n",
      "X (100317, 3000)\n",
      "batch_data (100317,)\n",
      "batch_indices (100317,)\n",
      "labels_all (100317,)\n"
     ]
    }
   ],
   "source": [
    "mask_sset = combined.obs[\"batch_names\"].isin(all_batch_indices).values\n",
    "labels_final = labels_f[mask_sset]\n",
    "X = combined.X[mask_sset]\n",
    "batch_data = combined.obs[\"batch_names\"][mask_sset]\n",
    "batch_indices, batches_mapper = pd.factorize(batch_data)\n",
    "\n",
    "print(\"Len of subset of indices :\", mask_sset.sum())\n",
    "\n",
    "if ON_LEAVES_ONLY:\n",
    "    n_labels_final = len(leaf_node_names) - 2\n",
    "    n_labels_hscanvi = len(leaf_node_names) - 2\n",
    "\n",
    "    mask_sset = (\n",
    "        combined.obs[\"batch_names\"].isin(all_batch_indices).values\n",
    "        & (labels_f < n_labels_final).values\n",
    "    )\n",
    "    labels_final = labels_f[mask_sset]\n",
    "    X = combined.X[mask_sset]\n",
    "    batch_data = combined.obs[\"batch_names\"][mask_sset]\n",
    "    batch_indices, batches_mapper = pd.factorize(batch_data)\n",
    "\n",
    "    where_eval = (labels_final.values < n_labels_final) & (\n",
    "        batch_data.isin(test_batch_indices).values\n",
    "    )\n",
    "\n",
    "    print(\"subsampled eval\", where_eval.sum())\n",
    "\n",
    "    where_leaves = (labels_final.values < n_labels_final) & (\n",
    "        batch_data.isin(test_batch_indices).values\n",
    "    )\n",
    "    where_leaves = where_leaves\n",
    "    where_eval = where_eval\n",
    "\n",
    "    labels_all = labels_final.values.copy()\n",
    "\n",
    "    labels_final = nodes_to_leaves_probs_cpu.numpy()[labels_final.values].argmax(-1)\n",
    "\n",
    "else:\n",
    "    labels = labels_final.values\n",
    "    where_eval = batch_data.isin(test_batch_indices).values\n",
    "    print(\"subsampled eval\", where_eval.sum())\n",
    "\n",
    "    n_labels_final = nodes_to_leaves_probs_cpu.shape[0]\n",
    "    n_labels_hscanvi = len(leaf_node_names) - 2\n",
    "\n",
    "    where_leaves = (labels_final.values < n_labels_hscanvi) & batch_data.isin(\n",
    "        test_batch_indices\n",
    "    ).values\n",
    "\n",
    "    labels_all = labels_final.values.copy()\n",
    "    labels_final = labels_final.values\n",
    "\n",
    "\n",
    "print(\"labels_final\", labels_final.shape)\n",
    "print(\"X\", X.shape)\n",
    "print(\"batch_data\", batch_data.shape)\n",
    "print(\"batch_indices\", batch_indices.shape)\n",
    "print(\"labels_all\", labels_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## construct scVI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-08-17 01:28:35,947] INFO - scvi.dataset.dataset | Remapping batch_indices to [0,N]\n",
      "[2020-08-17 01:28:35,954] INFO - scvi.dataset.dataset | Remapping labels to [0,N]\n"
     ]
    }
   ],
   "source": [
    "train_data = GeneExpressionDataset()\n",
    "train_data.populate_from_data(\n",
    "    X=X, batch_indices=batch_indices, labels=labels_final,\n",
    ")\n",
    "\n",
    "train_data.cell_types = nodes_to_indices.index.values\n",
    "train_data.labels = labels_final\n",
    "train_data.n_labels = len(train_data.cell_types)\n",
    "\n",
    "labelled = np.where(\n",
    "    batch_data.isin(train_batch_indices)\n",
    ")[0]\n",
    "unlabelled = np.where(\n",
    "    ~batch_data.isin(train_batch_indices)\n",
    ")[0]\n",
    "\n",
    "# Subsampling labelled examples in the case where we impose balance\n",
    "if is_fair:\n",
    "    pops = pd.Series(labels_final[labelled], index=labelled)\n",
    "    n_examples_ct = (pops).groupby(pops).size()\n",
    "    n_examples_ct = np.minimum(n_examples_ct, N_EXAMPLES_MAX)\n",
    "\n",
    "    selected_indices = pops.groupby(pops).apply(\n",
    "        lambda x: x.sample(n_examples_ct[x.name],random_state=TEST_SPLIT_SEED).index.values\n",
    "    )\n",
    "    labelled = np.concatenate(selected_indices.values)\n",
    "    unlabelled = np.where(~np.isin(np.arange(len(train_data)), labelled))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying to set attribute `.obs` of view, copying.\n"
     ]
    }
   ],
   "source": [
    "annd = combined[mask_sset]\n",
    "# Train labels\n",
    "annd.obs[\"labelled\"] = np.isin(np.arange(len(annd)), labelled)\n",
    "annd.obs[\"unlabelled\"] = np.isin(np.arange(len(annd)), unlabelled)\n",
    "\n",
    "assert (annd.obs[\"labelled\"].sum() == labelled.shape[0]) & (\n",
    "    annd.obs[\"unlabelled\"].sum() == unlabelled.shape[0]\n",
    ")\n",
    "# Eval labels\n",
    "annd.obs[\"eval\"] = where_eval\n",
    "annd.obs[\"eval_leaves\"] = where_leaves\n",
    "\n",
    "# Memory efficiency\n",
    "annd.obs[\"labelled\"] = annd.obs[\"labelled\"].astype(\"category\")\n",
    "annd.obs[\"unlabelled\"] = annd.obs[\"unlabelled\"].astype(\"category\")\n",
    "annd.obs[\"eval\"] = annd.obs[\"eval\"].astype(\"category\")\n",
    "annd.obs[\"eval_leaves\"] = annd.obs[\"eval_leaves\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "annd.obs[\"gt\"] = labels_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.unique(train_data.batch_indices).shape[0] == len(all_batch_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "annd_file = os.path.join(\n",
    "    SAVE_DIR,\n",
    "    \"train{}_test{}_{}_{}3.h5ad\".format(\n",
    "        \"\".join(train_batch_indices),\n",
    "        \"\".join(test_batch_indices),\n",
    "        ON_LEAVES_ONLY,\n",
    "        TEST_SPLIT_SEED,\n",
    "    ),\n",
    ")\n",
    "\n",
    "annd.write_h5ad(annd_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_annd(new_key, annd, predictions):\n",
    "    new_id_key = \"predictions_ids_{}\".format(new_key)\n",
    "    new_node_key = \"predictions_nodes_{}\".format(new_key)\n",
    "    new_name_key = \"predictions_names_{}\".format(new_key)\n",
    "\n",
    "    annd.obs[new_id_key] = predictions\n",
    "    annd.obs[new_node_key] = indices_to_nodes[annd.obs[new_id_key]].values\n",
    "    annd.obs[new_name_key] = id_to_label[annd.obs[new_node_key]].values\n",
    "\n",
    "    annd.obs[new_id_key] = annd.obs[new_id_key].astype(\"category\")\n",
    "    annd.obs[new_node_key] = annd.obs[new_node_key].astype(\"category\")\n",
    "    annd.obs[new_name_key] = annd.obs[new_name_key].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the results are saved in two ways.\n",
    "First classification performance are incrementally added to the list `results`, that we will later convert to a DataFrame, that will be easily savable.\n",
    "Second, all the predictions are hard-saved in a h5ad output file, that include the original data, label predictions and latent predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scVI and scANVI\n",
    "### parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINER_KWARGS = dict(\n",
    "    n_epochs_classifier=0,\n",
    "    lr_classification=1e-3,\n",
    "    batch_size=500,\n",
    "    n_epochs_kl_warmup=1,\n",
    ")\n",
    "TRAIN_SCVI_KWARGS = dict(n_epochs=30, lr=1e-3)\n",
    "TRAIN_SCANVI_KWARGS_vanilla = dict(n_epochs=50, lr=1e-3)\n",
    "TRAIN_SCANVI_KWARGS = dict(n_epochs=50, lr=1e-3)\n",
    "\n",
    "\n",
    "CLASSIFIER_PARAMETERS = dict(n_hidden=128, n_layers=1, dropout_rate=0.5)\n",
    "SCANVI_KWARGS = dict(\n",
    "    n_layers=1,\n",
    "    n_latent=50,\n",
    "    symmetric_kl=True,\n",
    "    dispersion=\"gene\",\n",
    "    classifier_parameters=CLASSIFIER_PARAMETERS,\n",
    ")\n",
    "SCVI_KWARGS = dict(\n",
    "    n_layers=1, n_latent=50, dispersion=\"gene\",  # dropout_rate=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scVI + kNN (+ ARI Reference)\n",
    "\n",
    "Train scVI and generates the data latent space, that will serve as input for the kNN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(n_input=train_data.nb_genes, n_batch=train_data.n_batches, **SCVI_KWARGS)\n",
    "trainer = UnsupervisedTrainer(\n",
    "    vae,\n",
    "    train_data,\n",
    "    batch_size=128,\n",
    "    n_epochs_kl_warmup=5,\n",
    "    data_loader_kwargs={\"pin_memory\": False},\n",
    ")\n",
    "trainer.train(**TRAIN_SCVI_KWARGS)\n",
    "\n",
    "full_unsup = trainer.create_posterior(\n",
    "    vae, train_data, indices=np.arange(len(train_data)),\n",
    ")\n",
    "latent_unsup, _, _ = full_unsup.sequential().get_latent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scVI_filename = SAVE_DIR + \"scVI_batch{}_fair{}_{}\".format(\n",
    "    str(train_batch_indices), int(is_fair), init\n",
    ")\n",
    "\n",
    "torch.save(vae.state_dict(), scVI_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annd = combined[mask_sset].copy()\n",
    "annd.obsm[\"X_scvi\"] = latent_unsup\n",
    "\n",
    "sc.pp.neighbors(annd, n_neighbors=30, n_pcs=50, use_rep=\"X_scvi\")\n",
    "sc.tl.leiden(annd)\n",
    "_cluster_unsup = annd.obs[\"leiden\"].values.astype(int)\n",
    "cluster_unsup = _cluster_unsup[where_eval]\n",
    "annd.obs[\"cluster_id\"] = _cluster_unsup.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(algorithm=\"kd_tree\")\n",
    "latent_train = latent_unsup[labelled]\n",
    "labels_train = labels[labelled]\n",
    "knn.fit(latent_train, labels_train)\n",
    "y_knn_full = knn.predict(latent_unsup)\n",
    "update_annd(new_key=\"kNN\", annd=annd, predictions=y_knn_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from sklearn import neighbors\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import torch.distributions as db\n",
    "\n",
    "from scvi.inference import Posterior\n",
    "from scvi.inference import Trainer\n",
    "from scvi.inference.inference import UnsupervisedTrainer\n",
    "from scvi.inference.posterior import unsupervised_clustering_accuracy\n",
    "import sklearn.metrics as met\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class AnnotationPosterior(Posterior):\n",
    "    def __init__(self, *args, model_zl=False, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.model_zl = model_zl\n",
    "\n",
    "    def accuracy(self):\n",
    "        model, cls = (\n",
    "            (self.sampling_model, self.model)\n",
    "            if hasattr(self, \"sampling_model\")\n",
    "            else (self.model, None)\n",
    "        )\n",
    "        acc = compute_accuracy(model, self, classifier=cls, model_zl=self.model_zl)\n",
    "        logger.debug(\"Acc: %.4f\" % (acc))\n",
    "        return acc\n",
    "\n",
    "    accuracy.mode = \"max\"\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def hierarchical_accuracy(self):\n",
    "        all_y, all_y_pred = self.compute_predictions()\n",
    "        acc = np.mean(all_y == all_y_pred)\n",
    "\n",
    "        all_y_groups = np.array([self.model.labels_groups[y] for y in all_y])\n",
    "        all_y_pred_groups = np.array([self.model.labels_groups[y] for y in all_y_pred])\n",
    "        h_acc = np.mean(all_y_groups == all_y_pred_groups)\n",
    "\n",
    "        logger.debug(\"Hierarchical Acc : %.4f\\n\" % h_acc)\n",
    "        return acc\n",
    "\n",
    "    accuracy.mode = \"max\"\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def compute_predictions(self, soft=False):\n",
    "        \"\"\"\n",
    "        :return: the true labels and the predicted labels\n",
    "        :rtype: 2-tuple of :py:class:`numpy.int32`\n",
    "        \"\"\"\n",
    "        model, cls = (\n",
    "            (self.sampling_model, self.model)\n",
    "            if hasattr(self, \"sampling_model\")\n",
    "            else (self.model, None)\n",
    "        )\n",
    "        return compute_predictions(\n",
    "            model, self, classifier=cls, soft=soft, model_zl=self.model_zl\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def compute_predictions_full(self):\n",
    "        \"\"\"\n",
    "        :return: the true labels and the predicted labels\n",
    "        :rtype: 2-tuple of :py:class:`numpy.int32`\n",
    "        \"\"\"\n",
    "        model, cls = (\n",
    "            (self.sampling_model, self.model)\n",
    "            if hasattr(self, \"sampling_model\")\n",
    "            else (self.model, None)\n",
    "        )\n",
    "        return compute_predictions_full(\n",
    "            model, self, classifier=cls, model_zl=self.model_zl\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def unsupervised_classification_accuracy(self):\n",
    "        all_y, all_y_pred = self.compute_predictions()\n",
    "        uca = unsupervised_clustering_accuracy(all_y, all_y_pred)[0]\n",
    "        logger.debug(\"UCA : %.4f\" % (uca))\n",
    "        return uca\n",
    "\n",
    "    unsupervised_classification_accuracy.mode = \"max\"\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def nn_latentspace(self, posterior):\n",
    "        data_train, _, labels_train = self.get_latent()\n",
    "        data_test, _, labels_test = posterior.get_latent()\n",
    "        nn = KNeighborsClassifier()\n",
    "        nn.fit(data_train, labels_train)\n",
    "        score = nn.score(data_test, labels_test)\n",
    "        return score\n",
    "\n",
    "\n",
    "class ClassifierTrainer(Trainer):\n",
    "    r\"\"\"The ClassifierInference class for training a classifier either on the raw data or on top of the latent\n",
    "        space of another model (VAE, VAEC, SCANVI).\n",
    "\n",
    "    Args:\n",
    "        :model: A model instance from class ``VAE``, ``VAEC``, ``SCANVI``\n",
    "        :gene_dataset: A gene_dataset instance like ``CortexDataset()``\n",
    "        :train_size: The train size, either a float between 0 and 1 or and integer for the number of training samples\n",
    "            to use Default: ``0.8``.\n",
    "        :test_size: The test size, either a float between 0 and 1 or and integer for the number of test samples\n",
    "            to use Default: ``None``.\n",
    "        :sampling_model: Model with z_encoder with which to first transform data.\n",
    "        :sampling_zl: Transform data with sampling_model z_encoder and l_encoder and concat.\n",
    "        :\\**kwargs: Other keywords arguments from the general Trainer class.\n",
    "\n",
    "\n",
    "    Examples:\n",
    "        >>> gene_dataset = CortexDataset()\n",
    "        >>> vae = VAE(gene_dataset.nb_genes, n_batch=gene_dataset.n_batches * False,\n",
    "        ... n_labels=gene_dataset.n_labels)\n",
    "\n",
    "        >>> classifier = Classifier(vae.n_latent, n_labels=cortex_dataset.n_labels)\n",
    "        >>> trainer = ClassifierTrainer(classifier, gene_dataset, sampling_model=vae, train_size=0.5)\n",
    "        >>> trainer.train(n_epochs=20, lr=1e-3)\n",
    "        >>> trainer.test_set.accuracy()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        train_size=0.8,\n",
    "        test_size=None,\n",
    "        sampling_model=None,\n",
    "        sampling_zl=False,\n",
    "        use_cuda=True,\n",
    "        nodes_to_leaves_probs=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.sampling_model = sampling_model\n",
    "        self.sampling_zl = sampling_zl\n",
    "        super().__init__(*args, use_cuda=use_cuda, **kwargs)\n",
    "        self.train_set, self.test_set, self.validation_set = self.train_test_validation(\n",
    "            self.model,\n",
    "            self.gene_dataset,\n",
    "            train_size=train_size,\n",
    "            test_size=test_size,\n",
    "            type_class=AnnotationPosterior,\n",
    "        )\n",
    "\n",
    "        self.nodes_to_leaves_probs = nodes_to_leaves_probs\n",
    "        self.train_set.to_monitor = [\"accuracy\"]\n",
    "        self.test_set.to_monitor = [\"accuracy\"]\n",
    "        self.validation_set.to_monitor = [\"accuracy\"]\n",
    "        self.train_set.model_zl = sampling_zl\n",
    "        self.test_set.model_zl = sampling_zl\n",
    "        self.validation_set.model_zl = sampling_zl\n",
    "\n",
    "    @property\n",
    "    def posteriors_loop(self):\n",
    "        return [\"train_set\"]\n",
    "\n",
    "    def __setattr__(self, key, value):\n",
    "        if key in [\"train_set\", \"test_set\"]:\n",
    "            value.sampling_model = self.sampling_model\n",
    "        super().__setattr__(key, value)\n",
    "\n",
    "    def loss(self, tensors_labelled):\n",
    "        x, _, _, _, labels_train = tensors_labelled\n",
    "        if self.sampling_model:\n",
    "            if hasattr(self.sampling_model, \"classify\"):\n",
    "                return F.cross_entropy(\n",
    "                    self.sampling_model.classify(x), labels_train.view(-1)\n",
    "                )\n",
    "            else:\n",
    "                if self.sampling_model.log_variational:\n",
    "                    x = torch.log(1 + x)\n",
    "                if self.sampling_zl:\n",
    "                    x_z = self.sampling_model.z_encoder(x)[0]\n",
    "                    x_l = self.sampling_model.l_encoder(x)[0]\n",
    "                    x = torch.cat((x_z, x_l), dim=-1)\n",
    "                else:\n",
    "                    x = self.sampling_model.z_encoder(x)[0]\n",
    "        return F.cross_entropy(self.model(x), labels_train.view(-1))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def compute_predictions(self, soft=False):\n",
    "        \"\"\"\n",
    "        :return: the true labels and the predicted labels\n",
    "        :rtype: 2-tuple of :py:class:`numpy.int32`\n",
    "        \"\"\"\n",
    "        model, cls = (\n",
    "            (self.sampling_model, self.model)\n",
    "            if hasattr(self, \"sampling_model\")\n",
    "            else (self.model, None)\n",
    "        )\n",
    "        full_set = self.create_posterior(type_class=AnnotationPosterior)\n",
    "        return compute_predictions(\n",
    "            model, full_set, classifier=cls, soft=soft, model_zl=self.sampling_zl\n",
    "        )\n",
    "\n",
    "    def on_training_loop(self, tensors_list):\n",
    "        if self.nodes_to_leaves_probs is not None:\n",
    "            new_tensors_list = self.convert_to_leaf_nodes(tensors_list[0])\n",
    "            new_tensors_list = [new_tensors_list]\n",
    "        else:\n",
    "            new_tensors_list = tensors_list\n",
    "        super().on_training_loop(new_tensors_list)\n",
    "        # self.current_loss = loss = self.loss(*tensors_list)\n",
    "        # self.optimizer.zero_grad()\n",
    "        # loss.backward()\n",
    "        # self.optimizer.step()\n",
    "\n",
    "    def convert_to_leaf_nodes(self, tensors):\n",
    "        sample_batch, b, c, d, y = tensors\n",
    "        if y is None:\n",
    "            return tensors\n",
    "        y_probs = self.nodes_to_leaves_probs[y]\n",
    "        leaves_batch = db.Categorical(probs=y_probs).sample()\n",
    "        new_tensors = (sample_batch, b, c, d, leaves_batch)\n",
    "        return new_tensors\n",
    "\n",
    "\n",
    "class SemiSupervisedTrainer(UnsupervisedTrainer):\n",
    "    r\"\"\"The SemiSupervisedTrainer class for the semi-supervised training of an autoencoder.\n",
    "    This parent class can be inherited to specify the different training schemes for semi-supervised learning\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        gene_dataset,\n",
    "        n_labels_final: int = None,\n",
    "        labels_of_use=None,\n",
    "        n_labelled_samples_per_class=50,\n",
    "        n_epochs_classifier=1,\n",
    "        lr_classification=5 * 1e-3,\n",
    "        classification_ratio=50,\n",
    "        seed=0,\n",
    "        nodes_to_leaves_probs: torch.Tensor = None,\n",
    "        indices_labelled: List = None,\n",
    "        indices_unlabelled: List = None,\n",
    "        include_conditionned_elbo: bool = False,\n",
    "        classify_full_ontology: bool = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param n_labelled_samples_per_class: number of labelled samples per class\n",
    "        \"\"\"\n",
    "        super().__init__(model, gene_dataset, **kwargs)\n",
    "        self.model = model\n",
    "        self.gene_dataset = gene_dataset\n",
    "        self.nodes_to_leaves_probs = nodes_to_leaves_probs\n",
    "\n",
    "        self.n_epochs_classifier = n_epochs_classifier\n",
    "        self.lr_classification = lr_classification\n",
    "        self.include_conditionned_elbo = include_conditionned_elbo\n",
    "        self.labelled_fraction = len(indices_labelled) / (1.0 * len(indices_unlabelled))\n",
    "        self.classification_ratio = classification_ratio\n",
    "        self.classify_full_ontology = classify_full_ontology\n",
    "\n",
    "        self.metrics = {\n",
    "            \"Heldout accuracy\": [],\n",
    "        }\n",
    "\n",
    "        if labels_of_use is None or n_labels_final is None:\n",
    "            labels_of_use = np.array(self.gene_dataset.labels).ravel()\n",
    "            n_labels_final = self.gene_dataset.n_labels\n",
    "\n",
    "        n_labelled_samples_per_class_array = [\n",
    "            n_labelled_samples_per_class\n",
    "        ] * n_labels_final\n",
    "        np.random.seed(seed=seed)\n",
    "        permutation_idx = np.random.permutation(len(labels_of_use))\n",
    "        labels_of_use_mix = labels_of_use[permutation_idx]\n",
    "        indices = []\n",
    "        current_nbrs = np.zeros(len(n_labelled_samples_per_class_array))\n",
    "        if (indices_labelled is None) or (indices_unlabelled is None):\n",
    "            for idx, (label) in enumerate(labels_of_use_mix):\n",
    "                label = int(label)\n",
    "                if label == -1:\n",
    "                    # print(\"Negative label, not included\")\n",
    "                    indices.append(idx)\n",
    "                elif current_nbrs[label] < n_labelled_samples_per_class_array[label]:\n",
    "                    indices.insert(0, idx)\n",
    "                    current_nbrs[label] += 1\n",
    "                else:\n",
    "                    indices.append(idx)\n",
    "            indices = np.array(indices)\n",
    "            total_labelled = sum(n_labelled_samples_per_class_array)\n",
    "            indices_labelled = permutation_idx[indices[:total_labelled]]\n",
    "            indices_unlabelled = permutation_idx[indices[total_labelled:]]\n",
    "\n",
    "        print(\n",
    "            \"labelled indices: \",\n",
    "            np.unique(gene_dataset.labels[indices_labelled].squeeze()),\n",
    "        )\n",
    "        print(\n",
    "            \"unlabelled indices: \",\n",
    "            np.unique(gene_dataset.labels[indices_unlabelled].squeeze()),\n",
    "        )\n",
    "\n",
    "        self.classifier_trainer = ClassifierTrainer(\n",
    "            model.classifier,\n",
    "            gene_dataset,\n",
    "            train_size=0.99,\n",
    "            nodes_to_leaves_probs=nodes_to_leaves_probs,\n",
    "            metrics_to_monitor=[],\n",
    "            show_progbar=False,\n",
    "            frequency=0,\n",
    "            sampling_model=self.model,\n",
    "        )\n",
    "        self.full_dataset = self.create_posterior(shuffle=True)\n",
    "        self.labelled_set = self.create_posterior(indices=indices_labelled)\n",
    "        self.unlabelled_set = self.create_posterior(indices=indices_unlabelled)\n",
    "\n",
    "        self.indices_labelled = indices_labelled\n",
    "        self.indices_unlabelled = indices_unlabelled\n",
    "\n",
    "        for posterior in [self.labelled_set, self.unlabelled_set]:\n",
    "            posterior.to_monitor = [\"reconstruction_error\", \"accuracy\"]\n",
    "\n",
    "    @property\n",
    "    def posteriors_loop(self):\n",
    "        return [\"full_dataset\", \"labelled_set\"]\n",
    "\n",
    "    def __setattr__(self, key, value):\n",
    "        if key == \"labelled_set\":\n",
    "            self.classifier_trainer.train_set = value\n",
    "        super().__setattr__(key, value)\n",
    "\n",
    "    def loss(self, tensors_all, tensors_labelled):\n",
    "        if self.include_conditionned_elbo:\n",
    "            unsup_loss = super().loss(tensors_all, feed_labels=False)\n",
    "            sup_loss = super().loss(tensors_labelled, feed_labels=True)\n",
    "            loss = unsup_loss + (self.labelled_fraction * sup_loss)\n",
    "        else:\n",
    "            loss = super().loss(tensors_all, feed_labels=False)\n",
    "        sample_batch, _, _, _, y = tensors_labelled\n",
    "\n",
    "        if not self.classify_full_ontology:\n",
    "            probs = self.model.classify(sample_batch)\n",
    "            y_gt = y.view(-1)\n",
    "            classification_loss = F.cross_entropy(probs, y_gt)\n",
    "        else:\n",
    "            # Compute all internal node labels\n",
    "            all_lbls = [y.view(-1)]\n",
    "            for a in self.model.ontology[::-1]:\n",
    "                A = torch.cuda.FloatTensor(a)\n",
    "                new_labels = A[:, all_lbls[-1].squeeze()].argmax(0)\n",
    "                all_lbls.append(new_labels)\n",
    "            all_lbls = all_lbls[::-1]\n",
    "\n",
    "            # Compute predictions\n",
    "            preds = []\n",
    "            for dep in range(1, self.model.depth + 1):\n",
    "                preds.append(self.model.classify(sample_batch, depth=dep))\n",
    "\n",
    "            # Compute classification loss\n",
    "            classification_loss = 0.0\n",
    "            for pred, lbl in zip(preds, all_lbls):\n",
    "                classification_loss += F.cross_entropy(pred, lbl)\n",
    "            classification_loss = classification_loss / len(preds)\n",
    "        # print(\"probs\", probs.min().item(), probs.max().item())\n",
    "        # print(\"y_gt\", y_gt.min().item(), y_gt.max().item())\n",
    "        loss += classification_loss * self.classification_ratio\n",
    "        # print(\"loss: \", loss.item())\n",
    "        # print(\"classification_loss: \", classification_loss.item())\n",
    "        # print(\"####################################################################################\")\n",
    "        return loss\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.model.eval()\n",
    "\n",
    "        if self.n_epochs_classifier != 0:\n",
    "            self.classifier_trainer.train(\n",
    "                self.n_epochs_classifier, lr=self.lr_classification\n",
    "            )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            full = self.create_posterior(\n",
    "                self.model, train_data, indices=np.arange(len(train_data))\n",
    "            )\n",
    "            gt, pred = full.sequential().compute_predictions()\n",
    "            acc = met.accuracy_score(gt[where_eval], pred[where_eval])\n",
    "            acc2 = met.balanced_accuracy_score(gt[where_eval], pred[where_eval])\n",
    "            print(acc, acc2)\n",
    "        #     self.metrics[\"Heldout accuracy\"].append(acc)\n",
    "        self.model.train()\n",
    "        return super().on_epoch_end()\n",
    "\n",
    "    def create_posterior(\n",
    "        self,\n",
    "        model=None,\n",
    "        gene_dataset=None,\n",
    "        shuffle=False,\n",
    "        indices=None,\n",
    "        type_class=AnnotationPosterior,\n",
    "    ):\n",
    "        return super().create_posterior(\n",
    "            model, gene_dataset, shuffle, indices, type_class\n",
    "        )\n",
    "\n",
    "    def on_training_loop(self, tensors_list):\n",
    "        # Modifies labels!!!\n",
    "        tensors_all, tensors_labelled = tensors_list\n",
    "        if self.nodes_to_leaves_probs is not None:\n",
    "            new_tensors_labelled = self.convert_to_leaf_nodes(tensors_labelled)\n",
    "            new_tensors_all = self.convert_to_leaf_nodes(tensors_all)\n",
    "        else:\n",
    "            new_tensors_labelled = tensors_labelled\n",
    "            new_tensors_all = tensors_all\n",
    "\n",
    "        self.current_loss = loss = self.loss(new_tensors_all, new_tensors_labelled)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def convert_to_leaf_nodes(self, tensors):\n",
    "        sample_batch, b, c, d, y = tensors\n",
    "        if y is None:\n",
    "            return tensors\n",
    "        y_probs = self.nodes_to_leaves_probs[y]\n",
    "        leaves_batch = db.Categorical(probs=y_probs).sample()\n",
    "        new_tensors = (sample_batch, b, c, d, leaves_batch)\n",
    "        return new_tensors\n",
    "\n",
    "\n",
    "class JointSemiSupervisedTrainer(SemiSupervisedTrainer):\n",
    "    def __init__(self, model, gene_dataset, **kwargs):\n",
    "        kwargs.update({\"n_epochs_classifier\": 0})\n",
    "        super().__init__(model, gene_dataset, **kwargs)\n",
    "\n",
    "\n",
    "class AlternateSemiSupervisedTrainer(SemiSupervisedTrainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def loss(self, all_tensor):\n",
    "        return UnsupervisedTrainer.loss(self, all_tensor)\n",
    "\n",
    "    @property\n",
    "    def posteriors_loop(self):\n",
    "        return [\"full_dataset\"]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_predictions(\n",
    "    model, data_loader, classifier=None, soft=False, model_zl=False, depth=None\n",
    "):\n",
    "    all_y_pred = []\n",
    "    all_y = []\n",
    "\n",
    "    for i_batch, tensors in enumerate(data_loader):\n",
    "        sample_batch, _, _, _, labels = tensors\n",
    "        all_y += [labels.view(-1).cpu()]\n",
    "\n",
    "        if hasattr(model, \"classify\"):\n",
    "            y_pred = model.classify(sample_batch, depth=depth)\n",
    "        elif classifier is not None:\n",
    "            # Then we use the specified classifier\n",
    "            if model is not None:\n",
    "                if model.log_variational:\n",
    "                    sample_batch = torch.log(1 + sample_batch)\n",
    "                if model_zl:\n",
    "                    sample_z = model.z_encoder(sample_batch)[0]\n",
    "                    sample_l = model.l_encoder(sample_batch)[0]\n",
    "                    sample_batch = torch.cat((sample_z, sample_l), dim=-1)\n",
    "                else:\n",
    "                    sample_batch, _, _ = model.z_encoder(sample_batch)\n",
    "            y_pred = classifier(sample_batch)\n",
    "        else:  # The model is the raw classifier\n",
    "            y_pred = model(sample_batch)\n",
    "\n",
    "        if not soft:\n",
    "            y_pred = y_pred.argmax(dim=-1)\n",
    "\n",
    "        all_y_pred += [y_pred.cpu()]\n",
    "\n",
    "    all_y_pred = np.array(torch.cat(all_y_pred))\n",
    "    all_y = np.array(torch.cat(all_y))\n",
    "\n",
    "    return all_y, all_y_pred\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_predictions_full(model, data_loader, classifier=None, model_zl=False):\n",
    "\n",
    "    # Compute predictions for each layer\n",
    "    all_preds = []\n",
    "    for dep in range(1, model.depth + 1):\n",
    "        all_y, all_y_pred = compute_predictions(\n",
    "            model, data_loader, classifier=None, soft=True, depth=dep\n",
    "        )\n",
    "        all_preds.append(all_y_pred)\n",
    "    return all_y, all_preds\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_accuracy(vae, data_loader, classifier=None, model_zl=False):\n",
    "    all_y, all_y_pred = compute_predictions(\n",
    "        vae, data_loader, classifier=classifier, model_zl=model_zl\n",
    "    )\n",
    "    return np.mean(all_y == all_y_pred)\n",
    "\n",
    "\n",
    "Accuracy = namedtuple(\n",
    "    \"Accuracy\", [\"unweighted\", \"weighted\", \"worst\", \"accuracy_classes\"]\n",
    ")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_accuracy_tuple(y, y_pred):\n",
    "    y = y.ravel()\n",
    "    n_labels = len(np.unique(y))\n",
    "    classes_probabilities = []\n",
    "    accuracy_classes = []\n",
    "    for cl in range(n_labels):\n",
    "        idx = y == cl\n",
    "        classes_probabilities += [np.mean(idx)]\n",
    "        accuracy_classes += [\n",
    "            np.mean((y[idx] == y_pred[idx])) if classes_probabilities[-1] else 0\n",
    "        ]\n",
    "        # This is also referred to as the \"recall\": p = n_true_positive / (n_false_negative + n_true_positive)\n",
    "        # ( We could also compute the \"precision\": p = n_true_positive / (n_false_positive + n_true_positive) )\n",
    "        accuracy_named_tuple = Accuracy(\n",
    "            unweighted=np.dot(accuracy_classes, classes_probabilities),\n",
    "            weighted=np.mean(accuracy_classes),\n",
    "            worst=np.min(accuracy_classes),\n",
    "            accuracy_classes=accuracy_classes,\n",
    "        )\n",
    "    return accuracy_named_tuple\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_accuracy_nn(data_train, labels_train, data_test, labels_test, k=5):\n",
    "    clf = neighbors.KNeighborsClassifier(k, weights=\"distance\")\n",
    "    return compute_accuracy_classifier(\n",
    "        clf, data_train, labels_train, data_test, labels_test\n",
    "    )\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_accuracy_classifier(clf, data_train, labels_train, data_test, labels_test):\n",
    "    clf.fit(data_train, labels_train)\n",
    "    # Predicting the labels\n",
    "    y_pred_test = clf.predict(data_test)\n",
    "    y_pred_train = clf.predict(data_train)\n",
    "\n",
    "    return (\n",
    "        (\n",
    "            compute_accuracy_tuple(labels_train, y_pred_train),\n",
    "            compute_accuracy_tuple(labels_test, y_pred_test),\n",
    "        ),\n",
    "        y_pred_test,\n",
    "    )\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_accuracy_svc(\n",
    "    data_train,\n",
    "    labels_train,\n",
    "    data_test,\n",
    "    labels_test,\n",
    "    param_grid=None,\n",
    "    verbose=0,\n",
    "    max_iter=-1,\n",
    "):\n",
    "    if param_grid is None:\n",
    "        param_grid = [\n",
    "            {\"C\": [1, 10, 100, 1000], \"kernel\": [\"linear\"]},\n",
    "            {\"C\": [1, 10, 100, 1000], \"gamma\": [0.001, 0.0001], \"kernel\": [\"rbf\"]},\n",
    "        ]\n",
    "    svc = SVC(max_iter=max_iter)\n",
    "    clf = GridSearchCV(svc, param_grid, verbose=verbose, cv=3)\n",
    "    return compute_accuracy_classifier(\n",
    "        clf, data_train, labels_train, data_test, labels_test\n",
    "    )\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_accuracy_rf(\n",
    "    data_train, labels_train, data_test, labels_test, param_grid=None, verbose=0\n",
    "):\n",
    "    if param_grid is None:\n",
    "        param_grid = {\"max_depth\": np.arange(3, 10), \"n_estimators\": [10, 50, 100, 200]}\n",
    "    rf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    clf = GridSearchCV(rf, param_grid, verbose=verbose, cv=3)\n",
    "    return compute_accuracy_classifier(\n",
    "        clf, data_train, labels_train, data_test, labels_test\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hscANVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labelled indices:  [  0   1   2   3   5   6   7  10  11  12  13  14  15  16  17  18  19  20\n",
      "  21  22  23  24  25  26  28  29  30  32  33  35  36  37  38  39  40  41\n",
      "  42  43  44  45  46  47 134]\n",
      "unlabelled indices:  [  0   1   2   3   5   6   7  10  11  12  15  16  17  19  20  21  22  23\n",
      "  24  25  28  29  30  32  33  35  36  37  38  39  40  41  42  44  45  46\n",
      "  47  84 132 134]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d42b36c66d4e1ebd9ef33e011a559c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/yosef2/users/pierreboyeau/miniconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1814: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6260001584409411 0.33024461504026553\n",
      "0.6292878079695794 0.375659255514279\n",
      "0.6171670759724313 0.4419814600720896\n",
      "0.6957141725421849 0.48445512489194453\n",
      "0.764952863820011 0.48925036389659615\n",
      "0.8459161847421374 0.5478673625444642\n",
      "0.8672264913253585 0.5626630210658505\n",
      "0.8757426919115899 0.5674472387858178\n",
      "0.8928147033193378 0.5787485766226788\n",
      "0.9092925611978135 0.5988183555455147\n",
      "0.8965776756713935 0.5997975481859219\n",
      "0.9236710766061951 0.6190985580303681\n",
      "0.9170957775489187 0.5966416718562065\n",
      "0.9148776043729699 0.6208595676118334\n",
      "0.9173730491959122 0.605661504876602\n",
      "0.9107185296680662 0.6044580024631773\n",
      "0.9130555335498692 0.60347061152838\n",
      "0.8796641052047849 0.6176510720329184\n",
      "0.8852095381446565 0.622230258539734\n",
      "0.8899627663788323 0.6450257418614758\n",
      "0.8755842509704508 0.6383828839606885\n",
      "0.8611661253267845 0.6325387013118077\n",
      "0.8785946288520954 0.6544960639517295\n",
      "0.8702764794422879 0.6585527477895635\n",
      "0.8846946050859542 0.6620120694465187\n",
      "0.8668699992077953 0.6632927374003245\n",
      "0.8511447357997306 0.6519157459821213\n",
      "0.8553038105046344 0.6519122514771586\n",
      "0.8411233462726768 0.6472050257974828\n",
      "0.8106630753386675 0.6479836171476105\n",
      "0.8046819298106631 0.6467711979795155\n",
      "0.8465895587419789 0.6588471666130145\n",
      "0.8152578626317041 0.6576115355920504\n",
      "0.7878871900499089 0.6458164785919164\n",
      "0.8154163035728432 0.6490976470908585\n",
      "0.7752515249940585 0.6438673326426576\n",
      "0.8024637566347144 0.652894513362399\n",
      "0.7836488948744356 0.6432430748616664\n",
      "0.8231799096886635 0.6537462730918788\n",
      "0.793749504872059 0.6466790686144753\n",
      "0.7880060207557633 0.6334040458487277\n",
      "0.7828963004040244 0.6405138215202217\n"
     ]
    }
   ],
   "source": [
    "train_data.cell_types = nodes_to_indices.index.values\n",
    "train_data.labels = labels_all\n",
    "train_data.n_labels = len(train_data.cell_types)\n",
    "\n",
    "# resh contains all the evaluation metrics for this algorithmn\n",
    "resh = dict(batch=train_batch_indices, model=\"h-scanVI\", init=init)\n",
    "\n",
    "# Hierarchical training & eval\n",
    "scanvi = SCANVI(\n",
    "    train_data.nb_genes,\n",
    "    n_batch=train_data.n_batches,\n",
    "    n_labels=n_labels_hscanvi,\n",
    "    use_ontology=True,\n",
    "    ontology=adjmc,\n",
    "    provide_onto_info=False,\n",
    "    #     provide_onto_info=False,\n",
    "    **SCANVI_KWARGS,\n",
    ")\n",
    "trainer_scanvi = SemiSupervisedTrainer(\n",
    "    scanvi,\n",
    "    train_data,\n",
    "    indices_labelled=labelled,\n",
    "    indices_unlabelled=unlabelled,\n",
    "    classification_ratio=50.,\n",
    "    nodes_to_leaves_probs=nodes_to_leaves_probs,\n",
    "    include_conditionned_elbo=False,\n",
    "    classify_full_ontology=True,\n",
    "    **TRAINER_KWARGS,\n",
    ")\n",
    "trainer_scanvi.train(**TRAIN_SCANVI_KWARGS)\n",
    "\n",
    "h_filename = SAVE_DIR + \"hier_batch{}_fair{}_{}\".format(\n",
    "    str(train_batch_indices), int(is_fair), init\n",
    ")\n",
    "torch.save(scanvi.state_dict(), h_filename)\n",
    "full = trainer_scanvi.create_posterior(\n",
    "    trainer_scanvi.model, train_data, indices=np.arange(len(train_data))\n",
    ")\n",
    "gt, pred = full.sequential().compute_predictions()\n",
    "latent, _, _ = full.sequential().get_latent()\n",
    "\n",
    "# if ON_LEAVES_ONLY:\n",
    "predicted_ = pred\n",
    "# else:\n",
    "#     # Fancy procedure to predict internal nodes\n",
    "y_true = labels[where_eval]\n",
    "y_pred = predicted_[where_eval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt, pred = full.sequential().compute_predictions()\n",
    "# latent, _, _ = full.sequential().get_latent()\n",
    "\n",
    "# # if ON_LEAVES_ONLY:\n",
    "# predicted_ = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt, pred_soft = full.sequential().compute_predictions(soft=True)\n",
    "unacceptable_cts = []\n",
    "acceptable_cts = []\n",
    "\n",
    "for leaf, nodes_subset in tqdm(leaf_paths.iterrows()):\n",
    "    if nodes_to_indices[leaf] >= n_labels_hscanvi:\n",
    "        continue\n",
    "    ind_subset = nodes_to_indices[nodes_subset].values\n",
    "    cells_subselect = np.isin(labels_train, np.unique(ind_subset))\n",
    "    if cells_subselect.sum() >= 1:\n",
    "        acceptable_cts.append(nodes_to_indices[leaf])\n",
    "    else:\n",
    "        unacceptable_cts.append(nodes_to_indices[leaf])\n",
    "\n",
    "pred_soft[:, unacceptable_cts] = 0\n",
    "predicted_ = pred_soft.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # trainer_scanvi.model.eval()\n",
    "# trainer_scanvi.model.train()\n",
    "# with torch.no_grad():\n",
    "#     full = trainer_scanvi.create_posterior(\n",
    "#         trainer_scanvi.model, train_data, indices=np.where(where_eval)[0]\n",
    "#     )\n",
    "#     gt, pred = full.sequential().compute_predictions()\n",
    "#     preds = [full.sequential().compute_predictions(soft=True)[1] for _ in tqdm(range(100))]\n",
    "# preds = [pred[..., None] for pred in preds]\n",
    "# preds = np.concatenate(preds, -1)\n",
    "# pred = preds.sum(-1).argmax(-1)\n",
    "# preds_oh = preds.copy()\n",
    "# preds_oh = (preds_oh == preds_oh.max(1)[:, None]).astype(int).sum(-1)\n",
    "# pred = preds_oh.argmax(-1)\n",
    "\n",
    "# latent, _, _ = full.sequential().get_latent()\n",
    "\n",
    "# # if ON_LEAVES_ONLY:\n",
    "# predicted_ = pred\n",
    "# # else:\n",
    "# #     # Fancy procedure to predict internal nodes\n",
    "# # y_true = labels[where_eval]\n",
    "# # y_pred = predicted_[where_eval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamer = pd.Series(\n",
    "    id_to_label[indices_to_nodes].values, index=indices_to_nodes.index.astype(str)\n",
    ")\n",
    "\n",
    "tree_mapper = []\n",
    "for idx, node in (indices_to_nodes).iteritems():\n",
    "    _node = node\n",
    "    path = []\n",
    "    for mat in adjm[::-1]:\n",
    "        if _node in mat.columns:\n",
    "            anc = mat.index[mat.loc[:, _node].argmax()]\n",
    "        else:\n",
    "            anc = _node\n",
    "        path.append(anc)\n",
    "        _node = anc\n",
    "    tree_path = {\"d{}\".format(i): nod for i, nod in enumerate(path)}\n",
    "    tree_mapper.append(\n",
    "        dict(\n",
    "            node=node,\n",
    "            **tree_path,\n",
    "        )\n",
    "    )\n",
    "tree_mapper = pd.DataFrame(tree_mapper).set_index(\"node\").assign(orig=lambda x: x.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h_class_perfs = get_classification_performance(\n",
    "#     y_true=y_true, y_pred=y_pred, y_ari=cluster_unsup, labels_mapper=renamer,\n",
    "# )\n",
    "\n",
    "# y_true_leaves = y_true[y_true < n_labels_hscanvi]\n",
    "# y_pred_leaves = y_pred[y_true < n_labels_hscanvi]\n",
    "# leaves_class_perfs = get_classification_performance(\n",
    "#     y_true=y_true_leaves,\n",
    "#     y_pred=y_pred_leaves,\n",
    "#     y_ari=None,\n",
    "#     labels_mapper=renamer,\n",
    "#     prefix=\"Leaves. \",\n",
    "# )\n",
    "# resh = {**resh, **h_class_perfs, **leaves_class_perfs}\n",
    "\n",
    "\n",
    "# nodes_true = indices_to_nodes[y_true].values\n",
    "# nodes_pred = indices_to_nodes[y_pred].values\n",
    "# y_anc_true = tree_mapper.reindex(nodes_true)\n",
    "# y_anc_pred = tree_mapper.reindex(nodes_pred)\n",
    "# for depth in [\"d0\", \"d2\", \"d4\"]:\n",
    "#     h_parent_class_perfs = get_classification_performance(\n",
    "#         y_true=nodes_to_indices[y_anc_true[depth]].values,\n",
    "#         y_pred=nodes_to_indices[y_anc_pred[depth]].values,\n",
    "#         prefix=\"Anc. {} \".format(depth),\n",
    "#         y_ari=cluster_unsup,\n",
    "#         labels_mapper=renamer,\n",
    "#     )\n",
    "#     resh = {**resh, **h_parent_class_perfs}\n",
    "# results.append(resh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent kNN approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_train = latent[labelled]\n",
    "latent_eval = latent[where_eval]\n",
    "labels_train = train_data.labels.squeeze()[labelled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "leaf_paths = tree_mapper.loc[leaf_node_names]\n",
    "\n",
    "bottomsup_classifiers = dict()\n",
    "extended_predictions = dict()\n",
    "for leaf, nodes_subset in tqdm(leaf_paths.iterrows()):\n",
    "#     print(nodes_to_indices[leaf])\n",
    "    ind_subset = nodes_to_indices[nodes_subset].values\n",
    "    \n",
    "    cells_subselect = np.isin(labels_train, np.unique(ind_subset))\n",
    "    x_train = latent_train[cells_subselect]\n",
    "    y_train = labels_train[cells_subselect]\n",
    "#     knn = KNeighborsClassifier(n_neighbors=3, algorithm=\"kd_tree\")\n",
    "    knn = RandomForestClassifier()\n",
    "    print(ind_subset, cells_subselect.sum(), np.unique(y_train, return_counts=True))\n",
    "    if cells_subselect.sum() >= 1:\n",
    "        knn.fit(x_train, y_train)\n",
    "        bottomsup_classifiers[nodes_to_indices[leaf]] = knn\n",
    "        extended_predictions[nodes_to_indices[leaf]] = knn.predict(latent)\n",
    "    else:\n",
    "        _pred = nodes_to_indices[leaf] * np.ones(len(latent))\n",
    "        extended_predictions[nodes_to_indices[leaf]] = _pred\n",
    "        \n",
    "\n",
    "extended_predictions = pd.DataFrame(extended_predictions)\n",
    "predicted_all = extended_predictions.lookup(\n",
    "    list(extended_predictions.index), \n",
    "    predicted_\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_eval = train_data.labels.squeeze()[where_eval]\n",
    "get_classification_performance(labels_eval, predicted_all[where_eval])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "5 neighbors\n",
    "\n",
    "    {'ARI': 0.6756242870939023,\n",
    "     'Accuracy': 0.6399808181588097,\n",
    "     'Accuracy (weighted)': 0.502664518029026,\n",
    "     'wAccuracy2': 0.6399808181588097,\n",
    "     'F1 (macro)': 0.3839107657664729,\n",
    "     'F1 (weighted)': 0.6738910799565824,\n",
    "     'y_true': array([20, 20, 20, ..., 19, 19, 19], dtype=uint16),\n",
    "     'y_pred': array([20., 20., 20., ..., 19., 19., 19.]),\n",
    "     'poe': 0.07313076953817053}\n",
    "     \n",
    "     \n",
    "20 neighbors\n",
    "\n",
    "    {'ARI': 0.6737769771356849,\n",
    "     'Accuracy': 0.6376630123483102,\n",
    "     'Accuracy (weighted)': 0.5017265938647667,\n",
    "     'wAccuracy2': 0.6376630123483104,\n",
    "     'F1 (macro)': 0.38358085532087355,\n",
    "     'F1 (weighted)': 0.6727164574114042,\n",
    "     'y_true': array([20, 20, 20, ..., 19, 19, 19], dtype=uint16),\n",
    "     'y_pred': array([20., 20., 20., ..., 19., 19., 19.]),\n",
    "     'poe': 0.07586151776318419}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_annd(new_key=\"hscANVI (full)\", annd=annd, predictions=predicted_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_annd(new_key=\"hscANVI (leaves)\", annd=annd, predictions=predicted_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bottoms-up approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Fancy procedure\n",
    "# # hscanvi\n",
    "# labels_train = np.unique(train_data.labels.squeeze())\n",
    "# y_gt, y_preds = full.sequential().compute_predictions_full()\n",
    "# chil_to_parents = [np.arange(scanvi.ontology[-1].shape[-1])]\n",
    "# chil_to_parent0 = np.array([np.where(y == 1)[0][0] for y in scanvi.ontology[-1].T])\n",
    "# chil_to_parents.append(chil_to_parent0)\n",
    "# associated_children = []\n",
    "# for itx, ont in enumerate(scanvi.ontology[-2::-1]):\n",
    "#     print(ont.shape)\n",
    "#     chil_to_parent = np.array([np.where(y == 1)[0][0] for y in ont.T])\n",
    "#     res = chil_to_parent[chil_to_parent0]\n",
    "#     associated_children.append(res)\n",
    "#     print(res.shape, np.unique(res).shape)\n",
    "#     chil_to_parent0 = res.copy()\n",
    "#     chil_to_parents.append(res)\n",
    "\n",
    "# chil_to_parents = chil_to_parents[::-1]\n",
    "\n",
    "# y_preds_unif = []\n",
    "# for yp, mapper in zip(y_preds, chil_to_parents):\n",
    "#     print(yp.shape)\n",
    "#     y_preds_unif.append(yp[:, mapper][..., None])\n",
    "# y_preds_unif = np.concatenate(y_preds_unif, axis=-1)\n",
    "\n",
    "# y_preds_pred = y_preds_unif[..., -1].argmax(-1)[:, None, None]\n",
    "# y_preds_unif.shape\n",
    "\n",
    "# for p_thresh in [0.7, 0.9, 0.95]:\n",
    "#     y_probs_best_path = np.take_along_axis(y_preds_unif, y_preds_pred, 1).squeeze()\n",
    "#     predicted_leaf_node = indices_to_nodes[y_preds_pred.squeeze()].values\n",
    "#     predicted_path_nodes = tree_mapper.loc[predicted_leaf_node]\n",
    "#     selected_depth = (len(adjm) + 1) - (y_probs_best_path >= p_thresh).sum(1)\n",
    "#     selected_depth = np.clip(selected_depth, a_min=0, a_max=len(adjm))\n",
    "#     predicted_node = predicted_path_nodes.loc[\n",
    "#         :, [\"orig\", \"d0\", \"d1\", \"d2\", \"d3\", \"d4\"]\n",
    "#     ].values[np.arange(len(predicted_path_nodes)), selected_depth]\n",
    "#     predicted_multi = nodes_to_indices[predicted_node].values\n",
    "#     print(get_classification_performance(labels[where_eval], predicted_multi[where_eval]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_classification_performance(labels[where_eval], y_knn_full[where_eval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_classification_performance(labels[where_eval], predicted_[where_eval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(resh.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_annd(new_key=\"hscANVI (leaves)\", annd=annd, predictions=predicted_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scanvi_vanilla = SCANVI(\n",
    "    train_data.nb_genes,\n",
    "    n_batch=train_data.n_batches,\n",
    "    n_labels=n_labels_final,\n",
    "    use_ontology=False,\n",
    "    **SCANVI_KWARGS,\n",
    ")\n",
    "trainer_scanvi_vanilla = SemiSupervisedTrainer(\n",
    "    scanvi_vanilla,\n",
    "    train_data,\n",
    "    indices_labelled=labelled,\n",
    "    indices_unlabelled=unlabelled,\n",
    "    include_conditionned_elbo=True,\n",
    "    **TRAINER_KWARGS,\n",
    ")\n",
    "trainer_scanvi_vanilla.train(**TRAIN_SCANVI_KWARGS_vanilla)\n",
    "\n",
    "vanilla_filename = SAVE_DIR + \"vanilla_batch{}_fair{}_{}\".format(\n",
    "    str(train_batch_indices), int(is_fair), init\n",
    ")\n",
    "torch.save(scanvi_vanilla.state_dict(), vanilla_filename)\n",
    "\n",
    "full_vanilla = trainer_scanvi_vanilla.create_posterior(\n",
    "    trainer_scanvi_vanilla.model, train_data, indices=np.arange(len(train_data)),\n",
    ")\n",
    "(gt_vanilla, pred_vanilla,) = full_vanilla.sequential().compute_predictions()\n",
    "latent_vanilla, _, _ = full_vanilla.sequential().get_latent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_eval = train_data.labels.squeeze()[where_eval]\n",
    "get_classification_performance(labels_eval, pred_vanilla[where_eval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_annd(new_key=\"scANVI\", annd=annd, predictions=pred_vanilla)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scVI + scANVI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can ignore, corresponds to the original scANVI algorithm which does not use symmetrical KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scanvi = SCANVI(\n",
    "#     train_data.nb_genes,\n",
    "#     n_batch=train_data.n_batches,\n",
    "#     n_labels=n_labels_final,\n",
    "#     use_ontology=False,\n",
    "#     **SCANVI_KWARGS,\n",
    "# )\n",
    "# trainer_scanvi = SemiSupervisedTrainer(\n",
    "#     scanvi,\n",
    "#     train_data,\n",
    "#     indices_labelled=labelled,\n",
    "#     indices_unlabelled=unlabelled,\n",
    "#     include_conditionned_elbo=True,\n",
    "#     **TRAINER_KWARGS,\n",
    "# )\n",
    "# trainer_scanvi.model.load_state_dict(torch.load(scVI_filename), strict=False)\n",
    "# trainer_scanvi.model.eval()\n",
    "# trainer_scanvi.train(**TRAIN_SCANVI_KWARGS)\n",
    "\n",
    "# scanvi_filename = SAVE_DIR + \"scanvi_batch{}_fair{}_{}\".format(\n",
    "#     str(train_batch_indices), int(is_fair), init\n",
    "# )\n",
    "# torch.save(scanvi.state_dict(), scanvi_filename)\n",
    "\n",
    "# full = trainer_scanvi.create_posterior(\n",
    "#     trainer_scanvi.model, train_data, indices=np.arange(len(train_data)),\n",
    "# )\n",
    "# (gt, pred_scanvi,) = full.sequential().compute_predictions()\n",
    "# latent_scanvi, _, _ = full.sequential().get_latent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_annd(\n",
    "#     new_key=\"scVI_scANVI\", annd=annd, predictions=pred_scanvi,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.log1p(X)\n",
    "data_train = data[labelled]\n",
    "y_train = labels[labelled]\n",
    "svm = LinearSVC()\n",
    "svm.fit(data_train, y_train)\n",
    "y_svm_full = svm.predict(data)\n",
    "update_annd(new_key=\"SVM\", annd=annd, predictions=y_svm_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MetaClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resh contains all the evaluation metrics for this algorithmn\n",
    "y_meta = predict_meta(\n",
    "    preds=[pred_vanilla, pred_scanvi, y_knn_full, y_svm_full], n_classes=n_labels_final\n",
    ")\n",
    "\n",
    "update_annd(new_key=\"META\", annd=annd, predictions=y_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `.h5ad` save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SUPER IMPORTANT SECTION: Saves all predictions in H5AD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annd.obsm[\"vanilla_latent\"] = latent_vanilla\n",
    "annd.obsm[\"scanvi_latent\"] = latent_scanvi\n",
    "annd.obsm[\"scvi_latent\"] = latent_unsup\n",
    "\n",
    "sc.pp.neighbors(final_data,n_neighbors=30, n_pcs=50, use_rep='scvi_latent')\n",
    "sc.tl.umap(final_data)\n",
    "\n",
    "annd_file = os.path.join(\n",
    "    SAVE_DIR,\n",
    "    \"results1layer_train{}_test{}_leavesonly{}_{}3.h5ad\".format(\n",
    "        \"\".join(train_batch_indices),\n",
    "        \"\".join(test_batch_indices),\n",
    "        ON_LEAVES_ONLY,\n",
    "        TEST_SPLIT_SEED,\n",
    "    ),\n",
    ")\n",
    "annd.write_h5ad(annd_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_leaf = y_gt < n_labels_hscanvi\n",
    "\n",
    "# leaf_cells = np.random.choice(np.where(is_leaf)[0], 300)\n",
    "# internal_cells = np.random.choice(np.where(~is_leaf)[0], 300)\n",
    "\n",
    "# plt.hist(y_probs_best_path[is_leaf, -1], label=\"leaves\", alpha=0.5, density=True)\n",
    "# plt.hist(y_probs_best_path[~is_leaf, -1], label=\"internal\", alpha=0.5, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_ = ~is_leaf\n",
    "# y_pred_ = y_probs_best_path[:, -1] <= 0.9\n",
    "\n",
    "# print(met.classification_report(y_, y_pred_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths_n = os.path.join(\n",
    "#     SAVE_DIR,\n",
    "#     \"paths_eval{}_{}.pdf\".format(\n",
    "#         \"\".join(train_batch_indices), \"\".join(test_batch_indices),\n",
    "#     ),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_leaf = y_gt < n_labels_hscanvi\n",
    "# is_leaf = is_leaf & where_eval\n",
    "# is_internal = (~is_leaf) & where_eval\n",
    "\n",
    "# leaf_cells = np.random.choice(np.where(is_leaf)[0], 100)\n",
    "# internal_cells = np.random.choice(np.where(is_internal)[0], 100)\n",
    "\n",
    "# fig, axes = plt.subplots(ncols=2, sharex=True, sharey=True, figsize=(8, 3.5))\n",
    "# plt.xlabel(\"Tree depth\", fontsize=13)\n",
    "# plt.sca(axes[0])\n",
    "# plt.ylabel(\"Posterior Probability\", fontsize=13)\n",
    "# plt.title(\"Leaf cells\", fontsize=15)\n",
    "\n",
    "# _y_probs_best_path = y_probs_best_path[leaf_cells]\n",
    "# plt.plot(_y_probs_best_path.T)\n",
    "# _ = plt.plot(np.median(y_probs_best_path[is_leaf].T, 1), color=\"red\", linewidth=5)\n",
    "# plt.sca(axes[1])\n",
    "# plt.yscale(\"log\")\n",
    "# plt.title(\"Internal nodes cells\", fontsize=15)\n",
    "\n",
    "# _y_probs_best_path = y_probs_best_path[internal_cells]\n",
    "# plt.plot(_y_probs_best_path.T)\n",
    "# _ = plt.plot(np.median(y_probs_best_path[~is_leaf].T, 1), color=\"red\", linewidth=5)\n",
    "\n",
    "# plt.suptitle(\"Predictions with hscANVI\", fontsize=16)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(paths_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pathsumap_n = \"pathsumap_eval{}_{}.pdf\".format(\n",
    "#     \"\".join(train_batch_indices), \"\".join(test_batch_indices),\n",
    "# )\n",
    "\n",
    "\n",
    "# # pathsumap_n = os.path.join(\n",
    "# #     SAVE_DIR,\n",
    "# #     \"pathsumap_eval{}_{}.pdf\".format(\n",
    "# #         \"\".join(train_batch_indices), \"\".join(test_batch_indices),\n",
    "# #     ),\n",
    "# # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pathsumap_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_gt_vanilla, y_preds_vanilla = full_vanilla.sequential().compute_predictions(soft=True)\n",
    "\n",
    "\n",
    "# annd.obsm[\"logp_path\"] = np.log(y_probs_best_path)\n",
    "# annd.obsm[\"p_path\"] = y_probs_best_path\n",
    "\n",
    "# y_true = labels\n",
    "# nodes_true = indices_to_nodes[y_true].values\n",
    "# y_anc_true = tree_mapper.reindex(nodes_true)\n",
    "\n",
    "# keys = []\n",
    "# for depth in [\"orig\", \"d2\", \"d4\"]:\n",
    "#     annd.obs[depth] = id_to_label[y_anc_true[depth]].values\n",
    "#     annd.obs[depth] = annd.obs[depth].astype(\"category\")\n",
    "#     keys.append(depth)\n",
    "\n",
    "# annd.obs[\"logp_max\"] = np.log(y_probs_best_path).max(-1)\n",
    "# keys.append(\"logp_max\")\n",
    "\n",
    "# annd.obs[\"is_leaf\"] = labels < n_labels_hscanvi\n",
    "# keys.append(\"is_leaf\")\n",
    "\n",
    "# anndeval = annd[where_eval]\n",
    "# sc.pp.neighbors(anndeval, n_neighbors=20, use_rep=\"p_path\")\n",
    "# sc.tl.umap(anndeval, min_dist=0.1)\n",
    "\n",
    "# sc.pl.umap(\n",
    "#     anndeval, color=keys, legend_fontsize=5, ncols=2, save=pathsumap_n,\n",
    "# )\n",
    "\n",
    "# res_study = pd.DataFrame(\n",
    "#     {\n",
    "#         \"Leaf probability\": (y_probs_best_path)[:, -1][where_eval],\n",
    "#         \"Probability (scANVI)\": y_preds_vanilla.max(-1)[where_eval],\n",
    "#         \"Cell-type\": anndeval.obs[\"orig\"],\n",
    "#         \"Ancestor\": anndeval.obs[\"d4\"],\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# _res_study = res_study.groupby(\"Cell-type\").mean()\n",
    "\n",
    "# import seaborn as sns\n",
    "\n",
    "# fig, axes = plt.subplots(ncols=2, figsize=(12, 6))\n",
    "# chart = sns.barplot(\n",
    "#     x=\"Cell-type\",\n",
    "#     y=\"Leaf probability\",\n",
    "#     order=_res_study.sort_values(\"Leaf probability\").index.values,\n",
    "#     data=res_study,\n",
    "#     ax=axes[0],\n",
    "# )\n",
    "# _ = chart.set_xticklabels(chart.get_xticklabels(), rotation=90)\n",
    "\n",
    "# chart = sns.barplot(\n",
    "#     x=\"Cell-type\",\n",
    "#     y=\"Probability (scANVI)\",\n",
    "#     order=_res_study.sort_values(\"Leaf probability\").index.values,\n",
    "#     data=res_study,\n",
    "#     ax=axes[1],\n",
    "# )\n",
    "# _ = chart.set_xticklabels(chart.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "233.567px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
